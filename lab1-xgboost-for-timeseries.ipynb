{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":11795033,"sourceType":"datasetVersion","datasetId":7406620}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<img src=\"images/header.png\" alt=\"Header\" style=\"width: 400px;\"/>","metadata":{}},{"cell_type":"markdown","source":"<h1 align=\"center\">Predictive Maintenance using NVIDIA RAPIDS and Deep Learning Models</h1>\n<h4 align=\"center\">Part 1: Training GPU XGBoost models with RAPIDS for Time Series</h4>\n\n","metadata":{}},{"cell_type":"markdown","source":"Working with time series algorithms can be difficult to configure and apply to arbitrary sequence prediction problems, even with well-defined and “easy to use” interfaces like those provided in the tf.keras deep learning library in Python.  One reason for this difficulty is the need for Deep Learning layers to input and return sequences rather than single values.\n\nIn this course, we will take a publicly available time series dataset of hard drive operational metrics provided by <a href=\"https://www.backblaze.com/b2/hard-drive-test-data.html\">Backblaze</a>, to build out and compare machine learning vs deep learning approaches to predictive maintenance problems. \n\nThe course materials are categorized in three sections. In this lab, we take a machine learning approach called “XGBoost” for classification of faulty/normal hard drives and discuss performance optimizations by utilizing accelerated computing. In lab 2, we move on to deep learning models and start with recurrent networks to predict the time series. We further explore variations of recurrent networks such as CNN-LSTM models. Lab 3, addresses applications of autoencoders to classify hard drives into faulty and normal bins. Throughout these labs, you will have the opportunity to get familiarized with details of implementations for each model, and learn how to curate the data and evaluate results. To begin, we start with \"Training GPU XGBoost models with RAPIDS for Time Series\". This lab covers the following topics:\n\n* [Background](#1)\n* [Predictive Maintenance: Predicting Anomalies Accurately and Early](#2)\n* [Lab Overview](#3)\n* [Dataset Loading and Pre-Processing](#4)\n* [Training GPU XGBoost models with RAPIDS for Time Series](#5)\n* [XGBoost](#6)\n* [More on Data Preparation](#7)\n    * [Exercise 1: Remove \"zero-valued\" columns](#e1)\n* [Define XGBoost Model Parameters](#8)\n* [Train XGBoost on CPUs](#9)\n* [Training with GPU without cuDF DataFrame](#10)\n* [Training with GPU and cuDF](#11)\n* [XGBoost Model Accuracy](#12)\n* [Measuring Model Accuracy](#13)\n* [Interpreting the results](#14)\n* [Dealing with Imbalanced Data](#15)\n    * [Exercise 2: Implications of balancing data samples](#e2)\n","metadata":{}},{"cell_type":"markdown","source":"<a name=\"1\"></a>\n## Background\n\nOne of the important recent topics of interest for industrial organizations  is to move away from reactive and schedule-based maintenance to <b>predictive maintenance</b> that helps optimize the operations and improve productivity.\n\n<a name=\"2\"></a>\n### Predictive Maintenance: Predicting Anomalies Accurately and Early\n\nWhereas schedule-based maintenance provides a regularly scheduled time for part replacements and repairs, predictive maintenance represents a condition based approach driven by data and analytics. Predictive maintenance strives to identify anomalies that can lead to costly breakdowns, such as improper machinery lubrication, contamination, misalignment, or suboptimal humidity and temperature conditions.\n\nBy identifying such anomalies of many part failures, estimating remaining useful life of these parts and then mapping anomalies to failure conditions by interpreting service logs, predictive maintenance helps to manage failures and avoid costly unplanned downtime. Other benefits include fewer unnecessary repairs, optimized spare parts inventory, and longer lifespans of equipment and parts—ultimately increasing equipment availability while improving operational efficiency. \n\nIn this regard, our goal in this lab is to show you how you can take your company's time series data and leverage it to predict outcomes.\n\n<a name=\"3\"></a>\n## Lab Overview\n\nOne of the common problems in a datacenter is failing data storage disks. Current business applications transfer large amounts of data and put very big pressure on the disks.  Using hard drive S.M.A.R.T. (Self-Monitoring Analysis and Reporting Technology) data provided from Backblaze, we will build conventional Machine Learning models in addition to Deep Learning methods to predict RUL (Remaining Useful Life) of a hard drive.\n\n- Dataset - https://www.backblaze.com/b2/hard-drive-test-data.html\n- Research paper from IBM - https://researcher.watson.ibm.com/researcher/files/us-mqiao/BigDataCongress_2018.pdf\n\nAfter completing this Lab, you will know how to take time series data and:\n\n- Predict Failure using a machine learning classification model with XGBoost\n- Predict Failure using a deep learning classification model using LSTM\n- Detect Anomalies using an Autoencoder (AE) or Seq2Seq model\n\n\nLet's start by importing some libraries used throughout this lab.","metadata":{}},{"cell_type":"code","source":"# Import libraries that will be needed for the Lab\nimport pandas as pd\nimport numpy as np\n\nimport sys\nimport glob\nimport os\nimport re\nimport polars as pl\nfrom pathlib import Path\n\nimport datetime\nimport time\nimport gc\nimport math\nimport random\nfrom random import shuffle, randrange\n\nfrom sklearn.utils import shuffle\nfrom sklearn import preprocessing\nfrom sklearn.utils import shuffle\nfrom sklearn.metrics import *\n\nimport matplotlib.pyplot as plt","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-14T05:45:44.644230Z","iopub.execute_input":"2025-05-14T05:45:44.644491Z","iopub.status.idle":"2025-05-14T05:45:44.650735Z","shell.execute_reply.started":"2025-05-14T05:45:44.644471Z","shell.execute_reply":"2025-05-14T05:45:44.649792Z"}},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"<a name=\"4\"></a>\n## Dataset Loading and Pre-Processing","metadata":{}},{"cell_type":"markdown","source":"The data set consists of a daily snapshot of each operational hard drive in the Backblaze datacenter at the time of measurement.  Each snapshot includes basic drive information along with S.M.A.R.T. statistics reported by that drive.\n","metadata":{}},{"cell_type":"code","source":"## Define location of our input and prepared data files\n# data_dir = './data/'\n\n# csv_train_file = 'Lab1-2017-Full_data.csv.gz'\n# csv_test_file = 'Lab1-2016-Q4_data.csv.gz'\n\n# pkl_train_file = 'Lab1-2017-Full-ST4000DM000.pkl'\n# pkl_test_file = 'Lab1-2016-Q4-ST4000DM000.pkl'","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Uitlize Polars for data loading","metadata":{}},{"cell_type":"code","source":"def load_all_csv_files_streaming(base_directory='.', year=2017, num_quarters=2, num_csv = None):\n    base_path = Path(base_directory)\n    matching_directories = list(base_path.glob(f'data_*_{str(year)}'))\n\n    if not matching_directories:\n        print(f\"No directories matching 'data_*_{str(year)}' found in {base_directory}\")\n        return None\n\n    def directory_sort_key(dir_path):\n        dir_name = dir_path.name\n        match = re.search(r'data_(.+)_{str(year)}', dir_name)\n        if match:\n            middle_part = match.group(1)\n            q_match = re.match(r'Q(\\d+)', middle_part)\n            if q_match:\n                return int(q_match.group(1))\n            return middle_part\n        return dir_name\n\n    sorted_directories = sorted(matching_directories, key=directory_sort_key)\n\n    print(f\"Found {len(sorted_directories)} matching directories (sorted):\")\n    for directory in sorted_directories:\n        print(f\"  - {directory}\")\n\n    combined_df = None\n    csv_count = 0\n    for directory in sorted_directories[:num_quarters]:\n        csv_files = sorted(directory.glob(\"*.csv\"))\n        print(f\"\\nLoading {len(csv_files)} CSV files from {directory}:\")\n\n        for file in csv_files:\n            try:\n                df = pl.read_csv(file)\n                if df.height == 0:\n                    print(f\"  - Skipped empty or all-NaN file: {file}\")\n                    continue\n\n                csv_count += 1\n                print(f\"  - Loaded {file.name}: ({df.height} rows, {df.width} columns)\")\n\n                if combined_df is None:\n                    combined_df = df\n                else:\n                    combined_df = pl.concat([combined_df, df], how=\"vertical_relaxed\", rechunk=True)\n                    \n                if num_csv and csv_count==num_csv:\n                    return combined_df\n                del df\n\n            except Exception as e:\n                print(f\"  - Error loading {file}: {str(e)}\")\n\n    print(f\"\\nSuccessfully loaded {csv_count} CSV files from {num_quarters} directories\")\n    return combined_df\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-14T05:47:54.567010Z","iopub.execute_input":"2025-05-14T05:47:54.567387Z","iopub.status.idle":"2025-05-14T05:47:54.577275Z","shell.execute_reply.started":"2025-05-14T05:47:54.567360Z","shell.execute_reply":"2025-05-14T05:47:54.576238Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"base_dir = \"/kaggle/input/backblaze-hard-drive-data-and-stats-2016-2017\"\ndf = load_all_csv_files_streaming(base_dir,num_quarters=1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-14T05:57:11.936383Z","iopub.execute_input":"2025-05-14T05:57:11.937095Z","iopub.status.idle":"2025-05-14T06:00:38.220356Z","shell.execute_reply.started":"2025-05-14T05:57:11.937067Z","shell.execute_reply":"2025-05-14T06:00:38.218869Z"}},"outputs":[{"name":"stdout","text":"Found 4 matching directories (sorted):\n  - /kaggle/input/backblaze-hard-drive-data-and-stats-2016-2017/data_Q1_2017\n  - /kaggle/input/backblaze-hard-drive-data-and-stats-2016-2017/data_Q2_2017\n  - /kaggle/input/backblaze-hard-drive-data-and-stats-2016-2017/data_Q3_2017\n  - /kaggle/input/backblaze-hard-drive-data-and-stats-2016-2017/data_Q4_2017\n\nLoading 90 CSV files from /kaggle/input/backblaze-hard-drive-data-and-stats-2016-2017/data_Q1_2017:\n  - Loaded 2017-01-01.csv: (73653 rows, 95 columns)\n  - Loaded 2017-01-02.csv: (73652 rows, 95 columns)\n  - Loaded 2017-01-03.csv: (73648 rows, 95 columns)\n  - Loaded 2017-01-04.csv: (73699 rows, 95 columns)\n  - Loaded 2017-01-05.csv: (73698 rows, 95 columns)\n  - Loaded 2017-01-06.csv: (73699 rows, 95 columns)\n  - Loaded 2017-01-07.csv: (73699 rows, 95 columns)\n  - Loaded 2017-01-08.csv: (73699 rows, 95 columns)\n  - Loaded 2017-01-09.csv: (73698 rows, 95 columns)\n  - Loaded 2017-01-10.csv: (73699 rows, 95 columns)\n  - Loaded 2017-01-11.csv: (73699 rows, 95 columns)\n  - Loaded 2017-01-12.csv: (73652 rows, 95 columns)\n  - Loaded 2017-01-13.csv: (73653 rows, 95 columns)\n  - Loaded 2017-01-14.csv: (73653 rows, 95 columns)\n  - Loaded 2017-01-15.csv: (73652 rows, 95 columns)\n  - Loaded 2017-01-16.csv: (73653 rows, 95 columns)\n  - Loaded 2017-01-17.csv: (73653 rows, 95 columns)\n  - Loaded 2017-01-18.csv: (73652 rows, 95 columns)\n  - Loaded 2017-01-19.csv: (73653 rows, 95 columns)\n  - Loaded 2017-01-20.csv: (73653 rows, 95 columns)\n  - Loaded 2017-01-21.csv: (73699 rows, 95 columns)\n  - Loaded 2017-01-22.csv: (73699 rows, 95 columns)\n  - Loaded 2017-01-23.csv: (73698 rows, 95 columns)\n  - Loaded 2017-01-24.csv: (73699 rows, 95 columns)\n  - Loaded 2017-01-25.csv: (73699 rows, 95 columns)\n  - Loaded 2017-01-26.csv: (73699 rows, 95 columns)\n  - Loaded 2017-01-27.csv: (73699 rows, 95 columns)\n  - Loaded 2017-01-28.csv: (46 rows, 95 columns)\n  - Loaded 2017-01-29.csv: (46 rows, 95 columns)\n  - Skipped empty or all-NaN file: /kaggle/input/backblaze-hard-drive-data-and-stats-2016-2017/data_Q1_2017/2017-01-30.csv\n  - Loaded 2017-01-31.csv: (61 rows, 95 columns)\n  - Loaded 2017-02-01.csv: (74980 rows, 95 columns)\n  - Loaded 2017-02-02.csv: (75041 rows, 95 columns)\n  - Loaded 2017-02-03.csv: (74995 rows, 95 columns)\n  - Loaded 2017-02-04.csv: (74995 rows, 95 columns)\n  - Loaded 2017-02-05.csv: (74994 rows, 95 columns)\n  - Loaded 2017-02-06.csv: (74934 rows, 95 columns)\n  - Loaded 2017-02-07.csv: (74934 rows, 95 columns)\n  - Loaded 2017-02-08.csv: (74934 rows, 95 columns)\n  - Loaded 2017-02-09.csv: (74933 rows, 95 columns)\n  - Loaded 2017-02-10.csv: (74933 rows, 95 columns)\n  - Loaded 2017-02-11.csv: (74500 rows, 95 columns)\n  - Loaded 2017-02-12.csv: (74933 rows, 95 columns)\n  - Loaded 2017-02-13.csv: (74917 rows, 95 columns)\n  - Loaded 2017-02-14.csv: (74933 rows, 95 columns)\n  - Loaded 2017-02-15.csv: (74933 rows, 95 columns)\n  - Loaded 2017-02-16.csv: (75850 rows, 95 columns)\n  - Loaded 2017-02-17.csv: (77690 rows, 95 columns)\n  - Loaded 2017-02-18.csv: (77694 rows, 95 columns)\n  - Loaded 2017-02-19.csv: (77727 rows, 95 columns)\n  - Loaded 2017-02-20.csv: (77727 rows, 95 columns)\n  - Loaded 2017-02-21.csv: (77631 rows, 95 columns)\n  - Loaded 2017-02-22.csv: (77692 rows, 95 columns)\n  - Loaded 2017-02-23.csv: (77754 rows, 95 columns)\n  - Loaded 2017-02-24.csv: (77754 rows, 95 columns)\n  - Loaded 2017-02-25.csv: (77755 rows, 95 columns)\n  - Loaded 2017-02-26.csv: (77750 rows, 95 columns)\n  - Loaded 2017-02-27.csv: (77755 rows, 95 columns)\n  - Loaded 2017-02-28.csv: (77693 rows, 95 columns)\n  - Loaded 2017-03-01.csv: (77752 rows, 95 columns)\n  - Loaded 2017-03-02.csv: (77754 rows, 95 columns)\n  - Loaded 2017-03-03.csv: (78674 rows, 95 columns)\n  - Loaded 2017-03-04.csv: (78674 rows, 95 columns)\n  - Loaded 2017-03-05.csv: (78674 rows, 95 columns)\n  - Loaded 2017-03-06.csv: (78674 rows, 95 columns)\n  - Loaded 2017-03-07.csv: (78674 rows, 95 columns)\n  - Loaded 2017-03-08.csv: (78674 rows, 95 columns)\n  - Loaded 2017-03-09.csv: (78613 rows, 95 columns)\n  - Loaded 2017-03-10.csv: (78612 rows, 95 columns)\n  - Loaded 2017-03-11.csv: (78613 rows, 95 columns)\n  - Loaded 2017-03-12.csv: (78612 rows, 95 columns)\n  - Loaded 2017-03-13.csv: (79833 rows, 95 columns)\n  - Loaded 2017-03-14.csv: (79833 rows, 95 columns)\n  - Loaded 2017-03-15.csv: (79833 rows, 95 columns)\n  - Loaded 2017-03-16.csv: (79832 rows, 95 columns)\n  - Loaded 2017-03-17.csv: (79832 rows, 95 columns)\n  - Loaded 2017-03-18.csv: (80993 rows, 95 columns)\n  - Loaded 2017-03-19.csv: (80947 rows, 95 columns)\n  - Loaded 2017-03-20.csv: (80941 rows, 95 columns)\n  - Loaded 2017-03-21.csv: (83386 rows, 95 columns)\n  - Loaded 2017-03-22.csv: (83386 rows, 95 columns)\n  - Loaded 2017-03-23.csv: (83387 rows, 95 columns)\n  - Loaded 2017-03-24.csv: (84607 rows, 95 columns)\n  - Loaded 2017-03-25.csv: (84605 rows, 95 columns)\n  - Loaded 2017-03-26.csv: (84467 rows, 95 columns)\n  - Loaded 2017-03-27.csv: (84464 rows, 95 columns)\n  - Loaded 2017-03-28.csv: (84530 rows, 95 columns)\n  - Loaded 2017-03-29.csv: (84467 rows, 95 columns)\n  - Loaded 2017-03-30.csv: (84469 rows, 95 columns)\n  - Loaded 2017-03-31.csv: (84469 rows, 95 columns)\n\nSuccessfully loaded 89 CSV files from 1 directories\n","output_type":"stream"}],"execution_count":17},{"cell_type":"markdown","source":"### check loading data","metadata":{}},{"cell_type":"code","source":"# Read entire SMART training data set\n# - Full 2017 dataset takes about 4 minutes to read in\n\nprint('Reading training data set...')\n# df = pd.read_csv(data_dir + csv_train_file)\n# print('Finished reading training data set')\nprint(df.shape)\ndf.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-14T05:52:13.592247Z","iopub.execute_input":"2025-05-14T05:52:13.592652Z","iopub.status.idle":"2025-05-14T05:52:13.619196Z","shell.execute_reply.started":"2025-05-14T05:52:13.592618Z","shell.execute_reply":"2025-05-14T05:52:13.618205Z"}},"outputs":[{"name":"stdout","text":"Reading training data set...\n","output_type":"stream"},{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"shape: (5, 95)\n┌───────────┬───────────┬───────────┬───────────┬───┬───────────┬───────────┬───────────┬──────────┐\n│ date      ┆ serial_nu ┆ model     ┆ capacity_ ┆ … ┆ smart_254 ┆ smart_254 ┆ smart_255 ┆ smart_25 │\n│ ---       ┆ mber      ┆ ---       ┆ bytes     ┆   ┆ _normaliz ┆ _raw      ┆ _normaliz ┆ 5_raw    │\n│ str       ┆ ---       ┆ str       ┆ ---       ┆   ┆ ed        ┆ ---       ┆ ed        ┆ ---      │\n│           ┆ str       ┆           ┆ i64       ┆   ┆ ---       ┆ str       ┆ ---       ┆ str      │\n│           ┆           ┆           ┆           ┆   ┆ str       ┆           ┆ str       ┆          │\n╞═══════════╪═══════════╪═══════════╪═══════════╪═══╪═══════════╪═══════════╪═══════════╪══════════╡\n│ 2017-01-0 ┆ MJ0351YNG ┆ Hitachi   ┆ 300059298 ┆ … ┆ null      ┆ null      ┆ null      ┆ null     │\n│ 1         ┆ 9Z0XA     ┆ HDS5C3030 ┆ 2016      ┆   ┆           ┆           ┆           ┆          │\n│           ┆           ┆ ALA630    ┆           ┆   ┆           ┆           ┆           ┆          │\n│ 2017-01-0 ┆ MJ0351YNG ┆ Hitachi   ┆ 300059298 ┆ … ┆ null      ┆ null      ┆ null      ┆ null     │\n│ 1         ┆ 9WJSA     ┆ HDS5C3030 ┆ 2016      ┆   ┆           ┆           ┆           ┆          │\n│           ┆           ┆ ALA630    ┆           ┆   ┆           ┆           ┆           ┆          │\n│ 2017-01-0 ┆ PL1321LAG ┆ Hitachi   ┆ 400078703 ┆ … ┆ null      ┆ null      ┆ null      ┆ null     │\n│ 1         ┆ 34XWH     ┆ HDS5C4040 ┆ 0016      ┆   ┆           ┆           ┆           ┆          │\n│           ┆           ┆ ALE630    ┆           ┆   ┆           ┆           ┆           ┆          │\n│ 2017-01-0 ┆ MJ0351YNG ┆ Hitachi   ┆ 300059298 ┆ … ┆ null      ┆ null      ┆ null      ┆ null     │\n│ 1         ┆ ABYAA     ┆ HDS5C3030 ┆ 2016      ┆   ┆           ┆           ┆           ┆          │\n│           ┆           ┆ ALA630    ┆           ┆   ┆           ┆           ┆           ┆          │\n│ 2017-01-0 ┆ Z305B2QN  ┆ ST4000DM0 ┆ 400078703 ┆ … ┆ null      ┆ null      ┆ null      ┆ null     │\n│ 1         ┆           ┆ 00        ┆ 0016      ┆   ┆           ┆           ┆           ┆          │\n└───────────┴───────────┴───────────┴───────────┴───┴───────────┴───────────┴───────────┴──────────┘","text/html":"<div><style>\n.dataframe > thead > tr,\n.dataframe > tbody > tr {\n  text-align: right;\n  white-space: pre-wrap;\n}\n</style>\n<small>shape: (5, 95)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>date</th><th>serial_number</th><th>model</th><th>capacity_bytes</th><th>failure</th><th>smart_1_normalized</th><th>smart_1_raw</th><th>smart_2_normalized</th><th>smart_2_raw</th><th>smart_3_normalized</th><th>smart_3_raw</th><th>smart_4_normalized</th><th>smart_4_raw</th><th>smart_5_normalized</th><th>smart_5_raw</th><th>smart_7_normalized</th><th>smart_7_raw</th><th>smart_8_normalized</th><th>smart_8_raw</th><th>smart_9_normalized</th><th>smart_9_raw</th><th>smart_10_normalized</th><th>smart_10_raw</th><th>smart_11_normalized</th><th>smart_11_raw</th><th>smart_12_normalized</th><th>smart_12_raw</th><th>smart_13_normalized</th><th>smart_13_raw</th><th>smart_15_normalized</th><th>smart_15_raw</th><th>smart_22_normalized</th><th>smart_22_raw</th><th>smart_183_normalized</th><th>smart_183_raw</th><th>smart_184_normalized</th><th>smart_184_raw</th><th>&hellip;</th><th>smart_197_raw</th><th>smart_198_normalized</th><th>smart_198_raw</th><th>smart_199_normalized</th><th>smart_199_raw</th><th>smart_200_normalized</th><th>smart_200_raw</th><th>smart_201_normalized</th><th>smart_201_raw</th><th>smart_220_normalized</th><th>smart_220_raw</th><th>smart_222_normalized</th><th>smart_222_raw</th><th>smart_223_normalized</th><th>smart_223_raw</th><th>smart_224_normalized</th><th>smart_224_raw</th><th>smart_225_normalized</th><th>smart_225_raw</th><th>smart_226_normalized</th><th>smart_226_raw</th><th>smart_240_normalized</th><th>smart_240_raw</th><th>smart_241_normalized</th><th>smart_241_raw</th><th>smart_242_normalized</th><th>smart_242_raw</th><th>smart_250_normalized</th><th>smart_250_raw</th><th>smart_251_normalized</th><th>smart_251_raw</th><th>smart_252_normalized</th><th>smart_252_raw</th><th>smart_254_normalized</th><th>smart_254_raw</th><th>smart_255_normalized</th><th>smart_255_raw</th></tr><tr><td>str</td><td>str</td><td>str</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>&hellip;</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>i64</td><td>i64</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td></tr></thead><tbody><tr><td>&quot;2017-01-01&quot;</td><td>&quot;MJ0351YNG9Z0XA&quot;</td><td>&quot;Hitachi HDS5C3030ALA630&quot;</td><td>3000592982016</td><td>0</td><td>100</td><td>0</td><td>135</td><td>108</td><td>127</td><td>554</td><td>100</td><td>15</td><td>100</td><td>0</td><td>100</td><td>0</td><td>122</td><td>37</td><td>95</td><td>36714</td><td>100</td><td>0</td><td>null</td><td>null</td><td>100</td><td>15</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>&hellip;</td><td>0</td><td>100</td><td>0</td><td>200</td><td>0</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td></tr><tr><td>&quot;2017-01-01&quot;</td><td>&quot;MJ0351YNG9WJSA&quot;</td><td>&quot;Hitachi HDS5C3030ALA630&quot;</td><td>3000592982016</td><td>0</td><td>100</td><td>0</td><td>136</td><td>104</td><td>126</td><td>555</td><td>100</td><td>21</td><td>100</td><td>2</td><td>100</td><td>0</td><td>122</td><td>37</td><td>95</td><td>36780</td><td>100</td><td>0</td><td>null</td><td>null</td><td>100</td><td>21</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>&hellip;</td><td>0</td><td>100</td><td>0</td><td>200</td><td>0</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td></tr><tr><td>&quot;2017-01-01&quot;</td><td>&quot;PL1321LAG34XWH&quot;</td><td>&quot;Hitachi HDS5C4040ALE630&quot;</td><td>4000787030016</td><td>0</td><td>100</td><td>0</td><td>134</td><td>101</td><td>130</td><td>560</td><td>100</td><td>31</td><td>100</td><td>0</td><td>100</td><td>0</td><td>113</td><td>42</td><td>96</td><td>33373</td><td>100</td><td>0</td><td>null</td><td>null</td><td>100</td><td>31</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>&hellip;</td><td>0</td><td>100</td><td>0</td><td>200</td><td>0</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td></tr><tr><td>&quot;2017-01-01&quot;</td><td>&quot;MJ0351YNGABYAA&quot;</td><td>&quot;Hitachi HDS5C3030ALA630&quot;</td><td>3000592982016</td><td>0</td><td>100</td><td>0</td><td>136</td><td>104</td><td>137</td><td>507</td><td>100</td><td>17</td><td>100</td><td>0</td><td>100</td><td>0</td><td>122</td><td>37</td><td>95</td><td>35424</td><td>100</td><td>0</td><td>null</td><td>null</td><td>100</td><td>17</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>&hellip;</td><td>0</td><td>100</td><td>0</td><td>200</td><td>0</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td></tr><tr><td>&quot;2017-01-01&quot;</td><td>&quot;Z305B2QN&quot;</td><td>&quot;ST4000DM000&quot;</td><td>4000787030016</td><td>0</td><td>113</td><td>58173272</td><td>null</td><td>null</td><td>91</td><td>0</td><td>100</td><td>8</td><td>100</td><td>0</td><td>85</td><td>388359773</td><td>null</td><td>null</td><td>90</td><td>9195</td><td>100</td><td>0</td><td>null</td><td>null</td><td>100</td><td>8</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>&quot;100&quot;</td><td>&quot;0&quot;</td><td>&quot;100&quot;</td><td>&quot;0&quot;</td><td>&hellip;</td><td>0</td><td>100</td><td>0</td><td>200</td><td>0</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>&quot;100&quot;</td><td>&quot;8947&quot;</td><td>&quot;100&quot;</td><td>&quot;30780539952&quot;</td><td>&quot;100&quot;</td><td>&quot;8290868549&quot;</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td></tr></tbody></table></div>"},"metadata":{}}],"execution_count":7},{"cell_type":"markdown","source":"## Data Set Overview<br>\n\nThe dataset in this DLI course is provided by Backblaze. More documentation can be found [here](https://www.backblaze.com/b2/hard-drive-test-data.html) about the type of the data and the different sensors which indicate the health of the hard drives. Following is a list of key parameters reported in the dataset. \n\n- <b>Date:</b> The date of the entrie recording in yyyy-mm-dd format.\n\n- <b>Serial Number:</b> The manufacturer-assigned serial number of the drive.\n\n- <b>Model:</b> The manufacturer-assigned model number of the drive.\n\n- <b>Capacity:</b> The drive capacity in bytes.\n\n- <b>Failure:</b> Contains a “0” if the drive is OK. Contains a “1” if this is the last day the drive was operational before failing.\n\n- <b>SMART Stats:</b> – 90 columns of data, which are the Raw and Normalized values for 45 different SMART stats as reported by the given drive. Each value is the number reported by the drive.\n","metadata":{}},{"cell_type":"markdown","source":"From the snapshot above, one important thing to notice is that we have several different models for which data is recorded. First, let's take a look at number of samples for each model:","metadata":{}},{"cell_type":"code","source":"# let's review the available hard drive models and select one with lot of samples for this exercise\n# print(df.groupby(\"model\").count().sort(\"count\", descending=True))\ndf['model'].value_counts().sort(\"count\", descending=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-14T05:53:08.961797Z","iopub.execute_input":"2025-05-14T05:53:08.962240Z","iopub.status.idle":"2025-05-14T05:53:09.330323Z","shell.execute_reply.started":"2025-05-14T05:53:08.962211Z","shell.execute_reply":"2025-05-14T05:53:09.329369Z"}},"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"shape: (58, 2)\n┌─────────────────────────┬─────────┐\n│ model                   ┆ count   │\n│ ---                     ┆ ---     │\n│ str                     ┆ u32     │\n╞═════════════════════════╪═════════╡\n│ ST4000DM000             ┆ 2981251 │\n│ HGST HMS5C4040BLE640    ┆ 959967  │\n│ ST8000DM002             ┆ 815494  │\n│ HGST HMS5C4040ALE640    ┆ 644282  │\n│ Hitachi HDS5C3030ALA630 ┆ 383788  │\n│ …                       ┆ …       │\n│ WDC WD3200LPVX          ┆ 86      │\n│ WDC WD1600BPVT          ┆ 86      │\n│ WDC WD10EADX            ┆ 59      │\n│ WDC WD10EARS            ┆ 32      │\n│ WDC WD1000FYPS          ┆ 11      │\n└─────────────────────────┴─────────┘","text/html":"<div><style>\n.dataframe > thead > tr,\n.dataframe > tbody > tr {\n  text-align: right;\n  white-space: pre-wrap;\n}\n</style>\n<small>shape: (58, 2)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>model</th><th>count</th></tr><tr><td>str</td><td>u32</td></tr></thead><tbody><tr><td>&quot;ST4000DM000&quot;</td><td>2981251</td></tr><tr><td>&quot;HGST HMS5C4040BLE640&quot;</td><td>959967</td></tr><tr><td>&quot;ST8000DM002&quot;</td><td>815494</td></tr><tr><td>&quot;HGST HMS5C4040ALE640&quot;</td><td>644282</td></tr><tr><td>&quot;Hitachi HDS5C3030ALA630&quot;</td><td>383788</td></tr><tr><td>&hellip;</td><td>&hellip;</td></tr><tr><td>&quot;WDC WD3200LPVX&quot;</td><td>86</td></tr><tr><td>&quot;WDC WD1600BPVT&quot;</td><td>86</td></tr><tr><td>&quot;WDC WD10EADX&quot;</td><td>59</td></tr><tr><td>&quot;WDC WD10EARS&quot;</td><td>32</td></tr><tr><td>&quot;WDC WD1000FYPS&quot;</td><td>11</td></tr></tbody></table></div>"},"metadata":{}}],"execution_count":9},{"cell_type":"markdown","source":"Because SMART stats usage can vary in meaning from manufacturer to manufacturer, we will limit our training data to a specific hard drive model, to reduce  potential of other manufactures' usage from influencing our model. We will pick the `ST4000DM000` model, which has the largest number of samples.","metadata":{}},{"cell_type":"code","source":"# ST4000DM000 is the most popular drive so let's use this to train\nharddrive_model = 'ST4000DM000'\n\n# keep only the selected hard drive model we want in our training data set\ndf = df.filter(pl.col(\"model\") == harddrive_model)\ndf.head()\n\n# # Load test set from Q2 data and pre-selected model\n# print('Reading test data set...')\n# df_t = pd.read_csv(data_dir + csv_test_file)\n# print('Finished reading test data set')\n\n# # keep only the selected hard drive model we want in our test data set\n# df_t = df_t[df_t.model == harddrive_model]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-14T06:06:34.126530Z","iopub.execute_input":"2025-05-14T06:06:34.127044Z","iopub.status.idle":"2025-05-14T06:06:36.434713Z","shell.execute_reply.started":"2025-05-14T06:06:34.127013Z","shell.execute_reply":"2025-05-14T06:06:36.433629Z"}},"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"shape: (5, 95)\n┌───────────┬───────────┬───────────┬───────────┬───┬───────────┬───────────┬───────────┬──────────┐\n│ date      ┆ serial_nu ┆ model     ┆ capacity_ ┆ … ┆ smart_254 ┆ smart_254 ┆ smart_255 ┆ smart_25 │\n│ ---       ┆ mber      ┆ ---       ┆ bytes     ┆   ┆ _normaliz ┆ _raw      ┆ _normaliz ┆ 5_raw    │\n│ str       ┆ ---       ┆ str       ┆ ---       ┆   ┆ ed        ┆ ---       ┆ ed        ┆ ---      │\n│           ┆ str       ┆           ┆ i64       ┆   ┆ ---       ┆ str       ┆ ---       ┆ str      │\n│           ┆           ┆           ┆           ┆   ┆ str       ┆           ┆ str       ┆          │\n╞═══════════╪═══════════╪═══════════╪═══════════╪═══╪═══════════╪═══════════╪═══════════╪══════════╡\n│ 2017-01-0 ┆ Z305B2QN  ┆ ST4000DM0 ┆ 400078703 ┆ … ┆ null      ┆ null      ┆ null      ┆ null     │\n│ 1         ┆           ┆ 00        ┆ 0016      ┆   ┆           ┆           ┆           ┆          │\n│ 2017-01-0 ┆ Z302A0YH  ┆ ST4000DM0 ┆ 400078703 ┆ … ┆ null      ┆ null      ┆ null      ┆ null     │\n│ 1         ┆           ┆ 00        ┆ 0016      ┆   ┆           ┆           ┆           ┆          │\n│ 2017-01-0 ┆ Z305BT0W  ┆ ST4000DM0 ┆ 400078703 ┆ … ┆ null      ┆ null      ┆ null      ┆ null     │\n│ 1         ┆           ┆ 00        ┆ 0016      ┆   ┆           ┆           ┆           ┆          │\n│ 2017-01-0 ┆ Z302A0YE  ┆ ST4000DM0 ┆ 400078703 ┆ … ┆ null      ┆ null      ┆ null      ┆ null     │\n│ 1         ┆           ┆ 00        ┆ 0016      ┆   ┆           ┆           ┆           ┆          │\n│ 2017-01-0 ┆ Z302PGH8  ┆ ST4000DM0 ┆ 400078703 ┆ … ┆ null      ┆ null      ┆ null      ┆ null     │\n│ 1         ┆           ┆ 00        ┆ 0016      ┆   ┆           ┆           ┆           ┆          │\n└───────────┴───────────┴───────────┴───────────┴───┴───────────┴───────────┴───────────┴──────────┘","text/html":"<div><style>\n.dataframe > thead > tr,\n.dataframe > tbody > tr {\n  text-align: right;\n  white-space: pre-wrap;\n}\n</style>\n<small>shape: (5, 95)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>date</th><th>serial_number</th><th>model</th><th>capacity_bytes</th><th>failure</th><th>smart_1_normalized</th><th>smart_1_raw</th><th>smart_2_normalized</th><th>smart_2_raw</th><th>smart_3_normalized</th><th>smart_3_raw</th><th>smart_4_normalized</th><th>smart_4_raw</th><th>smart_5_normalized</th><th>smart_5_raw</th><th>smart_7_normalized</th><th>smart_7_raw</th><th>smart_8_normalized</th><th>smart_8_raw</th><th>smart_9_normalized</th><th>smart_9_raw</th><th>smart_10_normalized</th><th>smart_10_raw</th><th>smart_11_normalized</th><th>smart_11_raw</th><th>smart_12_normalized</th><th>smart_12_raw</th><th>smart_13_normalized</th><th>smart_13_raw</th><th>smart_15_normalized</th><th>smart_15_raw</th><th>smart_22_normalized</th><th>smart_22_raw</th><th>smart_183_normalized</th><th>smart_183_raw</th><th>smart_184_normalized</th><th>smart_184_raw</th><th>&hellip;</th><th>smart_197_raw</th><th>smart_198_normalized</th><th>smart_198_raw</th><th>smart_199_normalized</th><th>smart_199_raw</th><th>smart_200_normalized</th><th>smart_200_raw</th><th>smart_201_normalized</th><th>smart_201_raw</th><th>smart_220_normalized</th><th>smart_220_raw</th><th>smart_222_normalized</th><th>smart_222_raw</th><th>smart_223_normalized</th><th>smart_223_raw</th><th>smart_224_normalized</th><th>smart_224_raw</th><th>smart_225_normalized</th><th>smart_225_raw</th><th>smart_226_normalized</th><th>smart_226_raw</th><th>smart_240_normalized</th><th>smart_240_raw</th><th>smart_241_normalized</th><th>smart_241_raw</th><th>smart_242_normalized</th><th>smart_242_raw</th><th>smart_250_normalized</th><th>smart_250_raw</th><th>smart_251_normalized</th><th>smart_251_raw</th><th>smart_252_normalized</th><th>smart_252_raw</th><th>smart_254_normalized</th><th>smart_254_raw</th><th>smart_255_normalized</th><th>smart_255_raw</th></tr><tr><td>str</td><td>str</td><td>str</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>&hellip;</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>i64</td><td>i64</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td></tr></thead><tbody><tr><td>&quot;2017-01-01&quot;</td><td>&quot;Z305B2QN&quot;</td><td>&quot;ST4000DM000&quot;</td><td>4000787030016</td><td>0</td><td>113</td><td>58173272</td><td>null</td><td>null</td><td>91</td><td>0</td><td>100</td><td>8</td><td>100</td><td>0</td><td>85</td><td>388359773</td><td>null</td><td>null</td><td>90</td><td>9195</td><td>100</td><td>0</td><td>null</td><td>null</td><td>100</td><td>8</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>&quot;100&quot;</td><td>&quot;0&quot;</td><td>&quot;100&quot;</td><td>&quot;0&quot;</td><td>&hellip;</td><td>0</td><td>100</td><td>0</td><td>200</td><td>0</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>&quot;100&quot;</td><td>&quot;8947&quot;</td><td>&quot;100&quot;</td><td>&quot;30780539952&quot;</td><td>&quot;100&quot;</td><td>&quot;8290868549&quot;</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td></tr><tr><td>&quot;2017-01-01&quot;</td><td>&quot;Z302A0YH&quot;</td><td>&quot;ST4000DM000&quot;</td><td>4000787030016</td><td>0</td><td>114</td><td>75626904</td><td>null</td><td>null</td><td>92</td><td>0</td><td>100</td><td>19</td><td>100</td><td>0</td><td>88</td><td>785458463</td><td>null</td><td>null</td><td>81</td><td>17043</td><td>100</td><td>0</td><td>null</td><td>null</td><td>100</td><td>19</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>&quot;100&quot;</td><td>&quot;0&quot;</td><td>&quot;100&quot;</td><td>&quot;0&quot;</td><td>&hellip;</td><td>0</td><td>100</td><td>0</td><td>200</td><td>0</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>&quot;100&quot;</td><td>&quot;16788&quot;</td><td>&quot;100&quot;</td><td>&quot;21648124248&quot;</td><td>&quot;100&quot;</td><td>&quot;162013891630&quot;</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td></tr><tr><td>&quot;2017-01-01&quot;</td><td>&quot;Z305BT0W&quot;</td><td>&quot;ST4000DM000&quot;</td><td>4000787030016</td><td>0</td><td>112</td><td>48893128</td><td>null</td><td>null</td><td>93</td><td>0</td><td>100</td><td>7</td><td>100</td><td>0</td><td>84</td><td>316494047</td><td>null</td><td>null</td><td>92</td><td>7857</td><td>100</td><td>0</td><td>null</td><td>null</td><td>100</td><td>7</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>&quot;100&quot;</td><td>&quot;0&quot;</td><td>&quot;100&quot;</td><td>&quot;0&quot;</td><td>&hellip;</td><td>0</td><td>100</td><td>0</td><td>200</td><td>0</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>&quot;100&quot;</td><td>&quot;7771&quot;</td><td>&quot;100&quot;</td><td>&quot;26168289344&quot;</td><td>&quot;100&quot;</td><td>&quot;15892146696&quot;</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td></tr><tr><td>&quot;2017-01-01&quot;</td><td>&quot;Z302A0YE&quot;</td><td>&quot;ST4000DM000&quot;</td><td>4000787030016</td><td>0</td><td>118</td><td>192617016</td><td>null</td><td>null</td><td>92</td><td>0</td><td>100</td><td>12</td><td>100</td><td>0</td><td>75</td><td>41700086</td><td>null</td><td>null</td><td>81</td><td>17362</td><td>100</td><td>0</td><td>null</td><td>null</td><td>100</td><td>12</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>&quot;1&quot;</td><td>&quot;4466&quot;</td><td>&quot;100&quot;</td><td>&quot;0&quot;</td><td>&hellip;</td><td>0</td><td>100</td><td>0</td><td>200</td><td>564</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>&quot;100&quot;</td><td>&quot;17308&quot;</td><td>&quot;100&quot;</td><td>&quot;21261063502&quot;</td><td>&quot;100&quot;</td><td>&quot;236386123294&quot;</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td></tr><tr><td>&quot;2017-01-01&quot;</td><td>&quot;Z302PGH8&quot;</td><td>&quot;ST4000DM000&quot;</td><td>4000787030016</td><td>0</td><td>114</td><td>83211536</td><td>null</td><td>null</td><td>92</td><td>0</td><td>100</td><td>20</td><td>100</td><td>0</td><td>87</td><td>640624192</td><td>null</td><td>null</td><td>85</td><td>13914</td><td>100</td><td>0</td><td>null</td><td>null</td><td>100</td><td>20</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>&quot;100&quot;</td><td>&quot;0&quot;</td><td>&quot;100&quot;</td><td>&quot;0&quot;</td><td>&hellip;</td><td>0</td><td>100</td><td>0</td><td>200</td><td>0</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>&quot;100&quot;</td><td>&quot;13560&quot;</td><td>&quot;100&quot;</td><td>&quot;16191633244&quot;</td><td>&quot;100&quot;</td><td>&quot;71986710022&quot;</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td></tr></tbody></table></div>"},"metadata":{}}],"execution_count":18},{"cell_type":"code","source":"# Load test set from Q2 data and pre-selected model\nprint('Reading test data set...')\n# df_t = pd.read_csv(data_dir + csv_test_file)\ndf_t = load_all_csv_files_streaming(base_dir,year=2016,num_csv=35)\nprint('Finished reading test data set')\n\n# keep only the selected hard drive model we want in our test data set\ndf_t = df_t.filter(pl.col(\"model\") == harddrive_model)\ndf_t.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-14T06:07:30.315171Z","iopub.execute_input":"2025-05-14T06:07:30.315515Z","iopub.status.idle":"2025-05-14T06:07:59.866902Z","shell.execute_reply.started":"2025-05-14T06:07:30.315489Z","shell.execute_reply":"2025-05-14T06:07:59.866052Z"}},"outputs":[{"name":"stdout","text":"Reading test data set...\nFound 1 matching directories (sorted):\n  - /kaggle/input/backblaze-hard-drive-data-and-stats-2016-2017/data_Q4_2016\n\nLoading 92 CSV files from /kaggle/input/backblaze-hard-drive-data-and-stats-2016-2017/data_Q4_2016:\n  - Loaded 2016-10-01.csv: (69302 rows, 95 columns)\n  - Loaded 2016-10-02.csv: (69302 rows, 95 columns)\n  - Loaded 2016-10-03.csv: (69302 rows, 95 columns)\n  - Loaded 2016-10-04.csv: (69302 rows, 95 columns)\n  - Loaded 2016-10-05.csv: (69302 rows, 95 columns)\n  - Loaded 2016-10-06.csv: (69302 rows, 95 columns)\n  - Loaded 2016-10-07.csv: (70522 rows, 95 columns)\n  - Loaded 2016-10-08.csv: (70522 rows, 95 columns)\n  - Loaded 2016-10-09.csv: (70476 rows, 95 columns)\n  - Loaded 2016-10-10.csv: (70475 rows, 95 columns)\n  - Loaded 2016-10-11.csv: (70415 rows, 95 columns)\n  - Loaded 2016-10-12.csv: (70414 rows, 95 columns)\n  - Loaded 2016-10-13.csv: (70415 rows, 95 columns)\n  - Loaded 2016-10-14.csv: (70476 rows, 95 columns)\n  - Loaded 2016-10-15.csv: (70476 rows, 95 columns)\n  - Loaded 2016-10-16.csv: (70476 rows, 95 columns)\n  - Loaded 2016-10-17.csv: (70476 rows, 95 columns)\n  - Loaded 2016-10-18.csv: (70475 rows, 95 columns)\n  - Loaded 2016-10-19.csv: (70476 rows, 95 columns)\n  - Loaded 2016-10-20.csv: (70476 rows, 95 columns)\n  - Loaded 2016-10-21.csv: (70475 rows, 95 columns)\n  - Loaded 2016-10-22.csv: (70384 rows, 95 columns)\n  - Loaded 2016-10-23.csv: (70335 rows, 95 columns)\n  - Loaded 2016-10-24.csv: (70291 rows, 95 columns)\n  - Loaded 2016-10-25.csv: (70338 rows, 95 columns)\n  - Loaded 2016-10-26.csv: (70338 rows, 95 columns)\n  - Loaded 2016-10-27.csv: (70338 rows, 95 columns)\n  - Loaded 2016-10-28.csv: (70337 rows, 95 columns)\n  - Loaded 2016-10-29.csv: (70384 rows, 95 columns)\n  - Loaded 2016-10-30.csv: (70384 rows, 95 columns)\n  - Loaded 2016-10-31.csv: (70384 rows, 95 columns)\n  - Loaded 2016-11-01.csv: (70333 rows, 95 columns)\n  - Loaded 2016-11-02.csv: (70338 rows, 95 columns)\n  - Loaded 2016-11-03.csv: (70338 rows, 95 columns)\n  - Loaded 2016-11-04.csv: (70338 rows, 95 columns)\nFinished reading test data set\n","output_type":"stream"},{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"shape: (5, 95)\n┌───────────┬───────────┬───────────┬───────────┬───┬───────────┬───────────┬───────────┬──────────┐\n│ date      ┆ serial_nu ┆ model     ┆ capacity_ ┆ … ┆ smart_254 ┆ smart_254 ┆ smart_255 ┆ smart_25 │\n│ ---       ┆ mber      ┆ ---       ┆ bytes     ┆   ┆ _normaliz ┆ _raw      ┆ _normaliz ┆ 5_raw    │\n│ str       ┆ ---       ┆ str       ┆ ---       ┆   ┆ ed        ┆ ---       ┆ ed        ┆ ---      │\n│           ┆ str       ┆           ┆ i64       ┆   ┆ ---       ┆ str       ┆ ---       ┆ str      │\n│           ┆           ┆           ┆           ┆   ┆ str       ┆           ┆ str       ┆          │\n╞═══════════╪═══════════╪═══════════╪═══════════╪═══╪═══════════╪═══════════╪═══════════╪══════════╡\n│ 2016-10-0 ┆ Z305B2QN  ┆ ST4000DM0 ┆ 400078703 ┆ … ┆ null      ┆ null      ┆ null      ┆ null     │\n│ 1         ┆           ┆ 00        ┆ 0016      ┆   ┆           ┆           ┆           ┆          │\n│ 2016-10-0 ┆ Z302A0YH  ┆ ST4000DM0 ┆ 400078703 ┆ … ┆ null      ┆ null      ┆ null      ┆ null     │\n│ 1         ┆           ┆ 00        ┆ 0016      ┆   ┆           ┆           ┆           ┆          │\n│ 2016-10-0 ┆ Z305BT0W  ┆ ST4000DM0 ┆ 400078703 ┆ … ┆ null      ┆ null      ┆ null      ┆ null     │\n│ 1         ┆           ┆ 00        ┆ 0016      ┆   ┆           ┆           ┆           ┆          │\n│ 2016-10-0 ┆ Z302A0YE  ┆ ST4000DM0 ┆ 400078703 ┆ … ┆ null      ┆ null      ┆ null      ┆ null     │\n│ 1         ┆           ┆ 00        ┆ 0016      ┆   ┆           ┆           ┆           ┆          │\n│ 2016-10-0 ┆ Z302PGH8  ┆ ST4000DM0 ┆ 400078703 ┆ … ┆ null      ┆ null      ┆ null      ┆ null     │\n│ 1         ┆           ┆ 00        ┆ 0016      ┆   ┆           ┆           ┆           ┆          │\n└───────────┴───────────┴───────────┴───────────┴───┴───────────┴───────────┴───────────┴──────────┘","text/html":"<div><style>\n.dataframe > thead > tr,\n.dataframe > tbody > tr {\n  text-align: right;\n  white-space: pre-wrap;\n}\n</style>\n<small>shape: (5, 95)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>date</th><th>serial_number</th><th>model</th><th>capacity_bytes</th><th>failure</th><th>smart_1_normalized</th><th>smart_1_raw</th><th>smart_2_normalized</th><th>smart_2_raw</th><th>smart_3_normalized</th><th>smart_3_raw</th><th>smart_4_normalized</th><th>smart_4_raw</th><th>smart_5_normalized</th><th>smart_5_raw</th><th>smart_7_normalized</th><th>smart_7_raw</th><th>smart_8_normalized</th><th>smart_8_raw</th><th>smart_9_normalized</th><th>smart_9_raw</th><th>smart_10_normalized</th><th>smart_10_raw</th><th>smart_11_normalized</th><th>smart_11_raw</th><th>smart_12_normalized</th><th>smart_12_raw</th><th>smart_13_normalized</th><th>smart_13_raw</th><th>smart_15_normalized</th><th>smart_15_raw</th><th>smart_22_normalized</th><th>smart_22_raw</th><th>smart_183_normalized</th><th>smart_183_raw</th><th>smart_184_normalized</th><th>smart_184_raw</th><th>&hellip;</th><th>smart_197_raw</th><th>smart_198_normalized</th><th>smart_198_raw</th><th>smart_199_normalized</th><th>smart_199_raw</th><th>smart_200_normalized</th><th>smart_200_raw</th><th>smart_201_normalized</th><th>smart_201_raw</th><th>smart_220_normalized</th><th>smart_220_raw</th><th>smart_222_normalized</th><th>smart_222_raw</th><th>smart_223_normalized</th><th>smart_223_raw</th><th>smart_224_normalized</th><th>smart_224_raw</th><th>smart_225_normalized</th><th>smart_225_raw</th><th>smart_226_normalized</th><th>smart_226_raw</th><th>smart_240_normalized</th><th>smart_240_raw</th><th>smart_241_normalized</th><th>smart_241_raw</th><th>smart_242_normalized</th><th>smart_242_raw</th><th>smart_250_normalized</th><th>smart_250_raw</th><th>smart_251_normalized</th><th>smart_251_raw</th><th>smart_252_normalized</th><th>smart_252_raw</th><th>smart_254_normalized</th><th>smart_254_raw</th><th>smart_255_normalized</th><th>smart_255_raw</th></tr><tr><td>str</td><td>str</td><td>str</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>&hellip;</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>str</td><td>str</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>str</td><td>str</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td></tr></thead><tbody><tr><td>&quot;2016-10-01&quot;</td><td>&quot;Z305B2QN&quot;</td><td>&quot;ST4000DM000&quot;</td><td>4000787030016</td><td>0</td><td>119</td><td>209710648</td><td>null</td><td>null</td><td>95</td><td>0</td><td>100</td><td>5</td><td>100</td><td>0</td><td>84</td><td>304354832</td><td>null</td><td>null</td><td>93</td><td>6986</td><td>100</td><td>0</td><td>null</td><td>null</td><td>100</td><td>5</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>100</td><td>0</td><td>100</td><td>0</td><td>&hellip;</td><td>0</td><td>100</td><td>0</td><td>200</td><td>0</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>100</td><td>6739</td><td>100</td><td>27184282600</td><td>100</td><td>5928789352</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td></tr><tr><td>&quot;2016-10-01&quot;</td><td>&quot;Z302A0YH&quot;</td><td>&quot;ST4000DM000&quot;</td><td>4000787030016</td><td>0</td><td>117</td><td>163793352</td><td>null</td><td>null</td><td>92</td><td>0</td><td>100</td><td>14</td><td>100</td><td>0</td><td>88</td><td>727905197</td><td>null</td><td>null</td><td>84</td><td>14835</td><td>100</td><td>0</td><td>null</td><td>null</td><td>100</td><td>14</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>100</td><td>0</td><td>100</td><td>0</td><td>&hellip;</td><td>0</td><td>100</td><td>0</td><td>200</td><td>0</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>100</td><td>14779</td><td>100</td><td>20932060064</td><td>100</td><td>149262396145</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td></tr><tr><td>&quot;2016-10-01&quot;</td><td>&quot;Z305BT0W&quot;</td><td>&quot;ST4000DM000&quot;</td><td>4000787030016</td><td>0</td><td>119</td><td>212355800</td><td>null</td><td>null</td><td>93</td><td>0</td><td>100</td><td>7</td><td>100</td><td>0</td><td>84</td><td>236396321</td><td>null</td><td>null</td><td>94</td><td>5648</td><td>100</td><td>0</td><td>null</td><td>null</td><td>100</td><td>7</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>100</td><td>0</td><td>100</td><td>0</td><td>&hellip;</td><td>0</td><td>100</td><td>0</td><td>200</td><td>0</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>100</td><td>5562</td><td>100</td><td>22290155664</td><td>100</td><td>12892380249</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td></tr><tr><td>&quot;2016-10-01&quot;</td><td>&quot;Z302A0YE&quot;</td><td>&quot;ST4000DM000&quot;</td><td>4000787030016</td><td>0</td><td>118</td><td>197520528</td><td>null</td><td>null</td><td>92</td><td>0</td><td>100</td><td>11</td><td>100</td><td>0</td><td>90</td><td>978330968</td><td>null</td><td>null</td><td>83</td><td>15153</td><td>100</td><td>0</td><td>null</td><td>null</td><td>100</td><td>11</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>1</td><td>4466</td><td>100</td><td>0</td><td>&hellip;</td><td>0</td><td>100</td><td>0</td><td>200</td><td>564</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>100</td><td>15111</td><td>100</td><td>20538193835</td><td>100</td><td>222053692697</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td></tr><tr><td>&quot;2016-10-01&quot;</td><td>&quot;Z302PGH8&quot;</td><td>&quot;ST4000DM000&quot;</td><td>4000787030016</td><td>0</td><td>119</td><td>222056512</td><td>null</td><td>null</td><td>92</td><td>0</td><td>100</td><td>20</td><td>100</td><td>0</td><td>87</td><td>567938712</td><td>null</td><td>null</td><td>87</td><td>11705</td><td>100</td><td>0</td><td>null</td><td>null</td><td>100</td><td>20</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>100</td><td>0</td><td>100</td><td>0</td><td>&hellip;</td><td>0</td><td>100</td><td>0</td><td>200</td><td>0</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>100</td><td>11351</td><td>100</td><td>15375868287</td><td>100</td><td>68020674044</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td></tr></tbody></table></div>"},"metadata":{}}],"execution_count":19},{"cell_type":"code","source":"#converting string to date time format for sorting and plotting time series data \n# df['date'] = pd.to_datetime(df['date'])\n# df_t['date'] = pd.to_datetime(df_t['date'])\ndf = df.with_columns(pl.col(\"date\").str.to_datetime())\ndf_t = df_t.with_columns(pl.col(\"date\").str.to_datetime())\nprint(df['date'].describe())\nprint(df_t['date'].describe())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-14T06:08:53.474096Z","iopub.execute_input":"2025-05-14T06:08:53.474441Z","iopub.status.idle":"2025-05-14T06:08:53.528299Z","shell.execute_reply.started":"2025-05-14T06:08:53.474416Z","shell.execute_reply":"2025-05-14T06:08:53.527128Z"}},"outputs":[{"name":"stdout","text":"shape: (8, 2)\n┌────────────┬────────────────────────────┐\n│ statistic  ┆ value                      │\n│ ---        ┆ ---                        │\n│ str        ┆ str                        │\n╞════════════╪════════════════════════════╡\n│ count      ┆ 2981251                    │\n│ null_count ┆ 0                          │\n│ mean       ┆ 2017-02-15 04:49:16.620291 │\n│ min        ┆ 2017-01-01 00:00:00        │\n│ 25%        ┆ 2017-01-22 00:00:00        │\n│ 50%        ┆ 2017-02-16 00:00:00        │\n│ 75%        ┆ 2017-03-10 00:00:00        │\n│ max        ┆ 2017-03-31 00:00:00        │\n└────────────┴────────────────────────────┘\nshape: (8, 2)\n┌────────────┬────────────────────────────┐\n│ statistic  ┆ value                      │\n│ ---        ┆ ---                        │\n│ str        ┆ str                        │\n╞════════════╪════════════════════════════╡\n│ count      ┆ 1216072                    │\n│ null_count ┆ 0                          │\n│ mean       ┆ 2016-10-18 00:00:03.978711 │\n│ min        ┆ 2016-10-01 00:00:00        │\n│ 25%        ┆ 2016-10-09 00:00:00        │\n│ 50%        ┆ 2016-10-18 00:00:00        │\n│ 75%        ┆ 2016-10-27 00:00:00        │\n│ max        ┆ 2016-11-04 00:00:00        │\n└────────────┴────────────────────────────┘\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"def get_cmap(n, name='hsv'):\n    \"\"\"Returns a function that maps each index in 0, 1, ..., n-1 to a distinct \n    RGB color; the keyword argument name must be a standard mpl colormap name.\"\"\"\n    return plt.cm.get_cmap(name, n)\n\ndef plot_df(df: pl.DataFrame, loc_col_list, axs):   \n    # Split DataFrame based on failure\n    df_normal = df.filter(pl.col(\"failure\") == 0)\n    df_failed = df.filter(pl.col(\"failure\") == 1)\n\n    n_rows = len(loc_col_list)\n    cmap = get_cmap(n_rows) \n    color_list = random.sample(range(n_rows), n_rows)\n    \n    for i, cur_col in enumerate(loc_col_list):\n        # Compute min/max\n        cur_min = df.select(pl.col(cur_col).min()).item()\n        cur_max = df.select(pl.col(cur_col).max()).item()\n\n        # Extract Series for plotting\n        date_normal = df_normal[\"date\"].to_pandas()\n        y_normal = df_normal[cur_col].to_pandas()\n        date_failed = df_failed[\"date\"].to_pandas()\n        y_failed = df_failed[cur_col].to_pandas()\n\n        # Plot\n        axs[i].plot(date_normal, y_normal, '.', color=cmap(color_list[i]), markersize=5, alpha=0.2)\n        axs[i].plot(date_failed, y_failed, '.', color=cmap(color_list[i]), markersize=20, alpha=0.6)\n        axs[i].set_ylabel(cur_col)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-14T05:54:18.167536Z","iopub.execute_input":"2025-05-14T05:54:18.168440Z","iopub.status.idle":"2025-05-14T05:54:18.177611Z","shell.execute_reply.started":"2025-05-14T05:54:18.168407Z","shell.execute_reply":"2025-05-14T05:54:18.176536Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"def remove_constant_values(df: pl.DataFrame, check_str: str):\n    col_list = df.columns\n    loc_col_list = []\n\n    print(\"Range for    : {:>25s} {:>15s}\".format(\"min\", \"max\"))\n\n    for cur_col in col_list:\n        if check_str in cur_col:\n            # Skip columns that are entirely null\n            null_count = df.select(pl.col(cur_col).null_count()).item()\n            if null_count == df.height:\n                continue\n\n            try:\n                cur_min = df.select(pl.col(cur_col).min()).item()\n                cur_max = df.select(pl.col(cur_col).max()).item()\n\n                # Ensure both values are not None (i.e., actual values exist)\n                if cur_min is not None and cur_max is not None and cur_min != cur_max:\n                    print(\"  {:20s}   {:15.3f} {:15.3f}\".format(cur_col, float(cur_min), float(cur_max)))\n                    loc_col_list.append(cur_col)\n            except Exception as e:\n                print(f\"  Skipped column {cur_col} due to error: {e}\")\n\n    return loc_col_list\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-14T06:11:25.934337Z","iopub.execute_input":"2025-05-14T06:11:25.935119Z","iopub.status.idle":"2025-05-14T06:11:25.942978Z","shell.execute_reply.started":"2025-05-14T06:11:25.935089Z","shell.execute_reply":"2025-05-14T06:11:25.942007Z"}},"outputs":[],"execution_count":23},{"cell_type":"markdown","source":"### Cleaning and understanding input data \nWhile building any machine learning/deep learning model, it is important to understand the quality and structure of the input data. For example, there can be missing data for several days for some of the sensors. There can also be sensors which do not have any data. The input data needs to be curated to remove such sensors and adjust the data for the sensors with missing values. ","metadata":{}},{"cell_type":"code","source":"# split into normal and failure dataframes\ndf_failed = df.filter(pl.col(\"failure\") == 1)\n# df_normal = df.filter(pl.col(\"failure\") == 0)\nprint(f\"num of failed rows: {len(df_failed)}, num of normal rows: {len(df_normal)}\")\n# col_list_raw = remove_constant_values(df, \"_raw\")\n# col_list_normalized = remove_constant_values(df, \"_normalized\")\n# df.select(col_list_normalized)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-14T06:18:52.325605Z","iopub.execute_input":"2025-05-14T06:18:52.326031Z","iopub.status.idle":"2025-05-14T06:18:56.171393Z","shell.execute_reply.started":"2025-05-14T06:18:52.326005Z","shell.execute_reply":"2025-05-14T06:18:56.170227Z"}},"outputs":[{"name":"stdout","text":"num of failed rows: 264, num of normal rows: 2980987\n","output_type":"stream"}],"execution_count":30},{"cell_type":"code","source":"# Remove constant-value SMART columns\nprint(f\"original columns: {len(df_failed.columns)}\")\ncol_list_raw = remove_constant_values(df_failed, \"_raw\")\ncol_list_normalized = remove_constant_values(df_failed, \"_normalized\")\n\n# Construct final list of columns to keep\ncol_list = [\"date\", \"serial_number\", \"model\", \"capacity_bytes\", \"failure\"]\ncol_list = col_list + col_list_raw + col_list_normalized\nprint(f\"filtered columns: {len(col_list)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-14T06:24:03.746424Z","iopub.execute_input":"2025-05-14T06:24:03.746806Z","iopub.status.idle":"2025-05-14T06:24:03.765457Z","shell.execute_reply.started":"2025-05-14T06:24:03.746782Z","shell.execute_reply":"2025-05-14T06:24:03.764331Z"}},"outputs":[{"name":"stdout","text":"original columns: 95\nRange for    :                       min             max\n  smart_1_raw                  20608.000   243309976.000\n  smart_4_raw                      1.000         176.000\n  smart_5_raw                      0.000       43248.000\n  smart_7_raw                      7.000 218859378419151.000\n  smart_9_raw                      0.000       33049.000\n  smart_12_raw                     0.000          70.000\n  smart_183_raw                    0.000         684.000\n  smart_184_raw                    0.000           8.000\n  smart_187_raw                    0.000           9.000\n  smart_188_raw                    0.000           6.000\n  smart_189_raw                    0.000           8.000\n  smart_190_raw                   14.000          38.000\n  smart_192_raw                    0.000         169.000\n  smart_193_raw                    5.000      313417.000\n  smart_194_raw                   14.000          38.000\n  smart_197_raw                    0.000         800.000\n  smart_198_raw                    0.000         800.000\n  smart_199_raw                    0.000          39.000\n  smart_240_raw                    0.000        9923.000\n  smart_241_raw                    0.000  9783419337.000\n  smart_242_raw          10294662021.000  9859081791.000\nRange for    :                       min             max\n  smart_1_normalized              79.000         120.000\n  smart_3_normalized              91.000         100.000\n  smart_5_normalized              67.000         100.000\n  smart_7_normalized              41.000         100.000\n  smart_9_normalized              63.000         100.000\n  smart_183_normalized             1.000          99.000\n  smart_184_normalized           100.000          99.000\n  smart_187_normalized             1.000          99.000\n  smart_189_normalized             1.000          99.000\n  smart_190_normalized            62.000          86.000\n  smart_193_normalized             1.000         100.000\n  smart_194_normalized            14.000          38.000\n  smart_197_normalized            96.000         100.000\n  smart_198_normalized            96.000         100.000\nfiltered columns: 40\n","output_type":"stream"}],"execution_count":36},{"cell_type":"code","source":"# Select only the relevant columns\ndf = df.select(col_list)\ndf_t = df_t.select(col_list)\n\n# Fill any remaining nulls with 0\ndf = df.fill_null(0)\ndf_t = df_t.fill_null(0)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-14T06:25:09.054701Z","iopub.execute_input":"2025-05-14T06:25:09.055028Z","iopub.status.idle":"2025-05-14T06:25:09.066612Z","shell.execute_reply.started":"2025-05-14T06:25:09.055006Z","shell.execute_reply":"2025-05-14T06:25:09.065648Z"}},"outputs":[],"execution_count":37},{"cell_type":"code","source":"# collecting the number of failed hard disks to compare the distributions of the sensor data with normal disks and also to plot time series data \n\nserial_num_list_failed = df_failed['serial_number'].value_counts()['serial_number'].to_list()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-14T06:28:08.415692Z","iopub.execute_input":"2025-05-14T06:28:08.416578Z","iopub.status.idle":"2025-05-14T06:28:08.424110Z","shell.execute_reply.started":"2025-05-14T06:28:08.416534Z","shell.execute_reply":"2025-05-14T06:28:08.423030Z"}},"outputs":[],"execution_count":43},{"cell_type":"code","source":"df_failed = df.filter(pl.col(\"failure\") == 1)\ndf_normal = df.filter(pl.col(\"failure\") == 0)\nprint(df_failed.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-14T06:32:25.351437Z","iopub.execute_input":"2025-05-14T06:32:25.351958Z","iopub.status.idle":"2025-05-14T06:32:26.443743Z","shell.execute_reply.started":"2025-05-14T06:32:25.351912Z","shell.execute_reply":"2025-05-14T06:32:26.442864Z"}},"outputs":[{"name":"stdout","text":"(264, 40)\n","output_type":"stream"}],"execution_count":45},{"cell_type":"markdown","source":"### Histogram of sensor data \nOnce the data is curated, a typical histogram plot of the data can reveal key operating points, outliers and provide further suggestions for preparing the data. The raw SMART sensor data is plotted below. ","metadata":{}},{"cell_type":"code","source":"n = 5 \nm = len(col_list_raw)//n \nif len(col_list_raw)%n > 0 : m += 1 \n    \nfig, axs = plt.subplots(figsize=(20,20), ncols=n, nrows=m)\naxs = axs.flatten()\n\nfor i,cur_col in enumerate(col_list_raw): \n    axs[i].hist(df_normal[cur_col],alpha=0.2,color=\"tab:blue\")\n    axs[i].hist(df_failed[cur_col],alpha=0.6,color=\"tab:orange\")\n    axs[i].set_yscale('log')\n    axs[i].set_title(cur_col)\n\nfor i in range(len(col_list_raw),axs.shape[0]): \n    axs[i].axis('off')\n\ni = len(col_list_raw)\naxs[i].plot([],[],alpha=0.2,color=\"tab:blue\",label=\"Normal disks\",linewidth=15)\naxs[i].plot([],[],alpha=0.6,color=\"tab:orange\",label=\"Failed disks\",linewidth=15)\naxs[i].legend(fontsize=16)\n    \nlen(col_list_raw)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-14T06:32:31.974914Z","iopub.execute_input":"2025-05-14T06:32:31.975318Z","execution_failed":"2025-05-14T06:51:56.497Z"}},"outputs":[{"execution_count":46,"output_type":"execute_result","data":{"text/plain":"21"},"metadata":{}},{"name":"stdout","text":"Error in callback <function _draw_all_if_interactive at 0x7a34e7a48ea0> (for post_execute):\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36m_draw_all_if_interactive\u001b[0;34m()\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_draw_all_if_interactive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_interactive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m         \u001b[0mdraw_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/matplotlib/_pylab_helpers.py\u001b[0m in \u001b[0;36mdraw_all\u001b[0;34m(cls, force)\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmanager\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_all_fig_managers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mforce\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mmanager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstale\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m                 \u001b[0mmanager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw_idle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/matplotlib/backend_bases.py\u001b[0m in \u001b[0;36mdraw_idle\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2080\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_idle_drawing\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2081\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_idle_draw_cntx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2082\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2083\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2084\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/matplotlib/backends/backend_agg.py\u001b[0m in \u001b[0;36mdraw\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    398\u001b[0m              (self.toolbar._wait_cursor_for_draw_cm() if self.toolbar\n\u001b[1;32m    399\u001b[0m               else nullcontext()):\n\u001b[0;32m--> 400\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    401\u001b[0m             \u001b[0;31m# A GUI class may be need to update a window using this draw, so\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    402\u001b[0m             \u001b[0;31m# don't forget to call the superclass.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/matplotlib/artist.py\u001b[0m in \u001b[0;36mdraw_wrapper\u001b[0;34m(artist, renderer, *args, **kwargs)\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdraw_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0martist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0martist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_rasterizing\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m             \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_rasterizing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/matplotlib/artist.py\u001b[0m in \u001b[0;36mdraw_wrapper\u001b[0;34m(artist, renderer)\u001b[0m\n\u001b[1;32m     70\u001b[0m                 \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0martist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0martist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_agg_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/matplotlib/figure.py\u001b[0m in \u001b[0;36mdraw\u001b[0;34m(self, renderer)\u001b[0m\n\u001b[1;32m   3173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3174\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3175\u001b[0;31m             mimage._draw_list_compositing_images(\n\u001b[0m\u001b[1;32m   3176\u001b[0m                 renderer, self, artists, self.suppressComposite)\n\u001b[1;32m   3177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/matplotlib/image.py\u001b[0m in \u001b[0;36m_draw_list_compositing_images\u001b[0;34m(renderer, parent, artists, suppress_composite)\u001b[0m\n\u001b[1;32m    129\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnot_composite\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhas_images\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0martists\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m             \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m         \u001b[0;31m# Composite any adjacent images together\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/matplotlib/artist.py\u001b[0m in \u001b[0;36mdraw_wrapper\u001b[0;34m(artist, renderer)\u001b[0m\n\u001b[1;32m     70\u001b[0m                 \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0martist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0martist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_agg_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36mdraw\u001b[0;34m(self, renderer)\u001b[0m\n\u001b[1;32m   3062\u001b[0m             \u001b[0m_draw_rasterized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0martists_rasterized\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3063\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3064\u001b[0;31m         mimage._draw_list_compositing_images(\n\u001b[0m\u001b[1;32m   3065\u001b[0m             renderer, self, artists, self.figure.suppressComposite)\n\u001b[1;32m   3066\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/matplotlib/image.py\u001b[0m in \u001b[0;36m_draw_list_compositing_images\u001b[0;34m(renderer, parent, artists, suppress_composite)\u001b[0m\n\u001b[1;32m    129\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnot_composite\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhas_images\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0martists\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m             \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m         \u001b[0;31m# Composite any adjacent images together\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/matplotlib/artist.py\u001b[0m in \u001b[0;36mdraw_wrapper\u001b[0;34m(artist, renderer)\u001b[0m\n\u001b[1;32m     70\u001b[0m                 \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0martist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0martist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_agg_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/matplotlib/axis.py\u001b[0m in \u001b[0;36mdraw\u001b[0;34m(self, renderer, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1374\u001b[0m         \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_gid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1375\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1376\u001b[0;31m         \u001b[0mticks_to_draw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_ticks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1377\u001b[0m         \u001b[0mtlb1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtlb2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_ticklabel_bboxes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mticks_to_draw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1378\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/matplotlib/axis.py\u001b[0m in \u001b[0;36m_update_ticks\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1262\u001b[0m         \u001b[0mmajor_locs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_majorticklocs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1263\u001b[0m         \u001b[0mmajor_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmajor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformatter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat_ticks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmajor_locs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1264\u001b[0;31m         \u001b[0mmajor_ticks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_major_ticks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmajor_locs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1265\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmajor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformatter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_locs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmajor_locs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1266\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtick\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmajor_ticks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmajor_locs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmajor_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/matplotlib/axis.py\u001b[0m in \u001b[0;36mget_major_ticks\u001b[0;34m(self, numticks)\u001b[0m\n\u001b[1;32m   1600\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmajorTicks\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mnumticks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1601\u001b[0m             \u001b[0;31m# Update the new tick label properties from the old.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1602\u001b[0;31m             \u001b[0mtick\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tick\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmajor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1603\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmajorTicks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtick\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1604\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_copy_tick_props\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmajorTicks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtick\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/matplotlib/axis.py\u001b[0m in \u001b[0;36m_get_tick\u001b[0;34m(self, major)\u001b[0m\n\u001b[1;32m   1549\u001b[0m                 \"_tick_class or reimplement _get_tick()\")\n\u001b[1;32m   1550\u001b[0m         \u001b[0mtick_kw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_major_tick_kw\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mmajor\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_minor_tick_kw\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1551\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tick_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmajor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmajor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mtick_kw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1552\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1553\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_tick_label_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/matplotlib/axis.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    422\u001b[0m         self.tick2line.set(\n\u001b[1;32m    423\u001b[0m             data=([0], [1]), transform=ax.get_xaxis_transform(\"tick2\"))\n\u001b[0;32m--> 424\u001b[0;31m         self.gridline.set(\n\u001b[0m\u001b[1;32m    425\u001b[0m             data=([0, 0], [0, 1]), transform=ax.get_xaxis_transform(\"grid\"))\n\u001b[1;32m    426\u001b[0m         \u001b[0;31m# the y loc is 3 points below the min of y axis\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/matplotlib/artist.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    145\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m         \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mArtist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    148\u001b[0m         \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"set\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m         \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__qualname__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"{cls.__qualname__}.set\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/matplotlib/artist.py\u001b[0m in \u001b[0;36mset\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m   1229\u001b[0m         \u001b[0;31m# Artist._update_set_signature_and_docstring() at the end of the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1230\u001b[0m         \u001b[0;31m# module.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1231\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_internal_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcbook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize_kwargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1232\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1233\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mcontextlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontextmanager\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/matplotlib/artist.py\u001b[0m in \u001b[0;36m_internal_update\u001b[0;34m(self, kwargs)\u001b[0m\n\u001b[1;32m   1221\u001b[0m         \u001b[0mThe\u001b[0m \u001b[0mlack\u001b[0m \u001b[0mof\u001b[0m \u001b[0mprenormalization\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mto\u001b[0m \u001b[0mmaintain\u001b[0m \u001b[0mbackcompatibility\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1222\u001b[0m         \"\"\"\n\u001b[0;32m-> 1223\u001b[0;31m         return self._update_props(\n\u001b[0m\u001b[1;32m   1224\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"{cls.__name__}.set() got an unexpected keyword argument \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1225\u001b[0m             \"{prop_name!r}\")\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/matplotlib/artist.py\u001b[0m in \u001b[0;36m_update_props\u001b[0;34m(self, props, errfmt)\u001b[0m\n\u001b[1;32m   1197\u001b[0m                         raise AttributeError(\n\u001b[1;32m   1198\u001b[0m                             errfmt.format(cls=type(self), prop_name=k))\n\u001b[0;32m-> 1199\u001b[0;31m                     \u001b[0mret\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1200\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1201\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpchanged\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/matplotlib/lines.py\u001b[0m in \u001b[0;36mset_data\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    649\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    650\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_xdata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 651\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_ydata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    652\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    653\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrecache_always\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/matplotlib/lines.py\u001b[0m in \u001b[0;36mset_ydata\u001b[0;34m(self, y)\u001b[0m\n\u001b[1;32m   1306\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_yorig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1307\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_invalidy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1308\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstale\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1309\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1310\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mset_dashes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/matplotlib/artist.py\u001b[0m in \u001b[0;36mstale\u001b[0;34m(self, val)\u001b[0m\n\u001b[1;32m    317\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stale\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m     \u001b[0;34m@\u001b[0m\u001b[0mstale\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    320\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stale\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":null},{"cell_type":"markdown","source":"### Time series visualization \n\nOf all the disks with available data, several disks fail. Since the SMART sensor data is time series, it is useful to visualize the sensor behavior for the failed disks. In the plots below, the raw sensor data are displayed for 10 failed disks out of the 1061 failed disks. This can give us insights to the behavior of the disk until the time of failure. ","metadata":{}},{"cell_type":"code","source":"from pandas.plotting import register_matplotlib_converters\nregister_matplotlib_converters()\n\n\nn_rows = len(col_list_raw)\nfig, axs = plt.subplots(ncols=1, nrows=n_rows, \n                            figsize=(10,1.5*n_rows),sharex=True)\nfor cur_num in serial_num_list_failed[:10]: # [:1]: \n    plot_df(df[df['serial_number']==cur_num],col_list_raw,axs)\n\nprint(\"NOTE : \\n    The large circles represent time of failure.\\n    The lines show the data leading up to the point of faiilure. \\n    Only serial numbers of failed disks are being used.\\n\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data_dir = \"/kaggle/working/\"\n# Save datasets for future use\npkl_train_file = 'Lab1-2017-Q1-ST4000DM000.pkl'\npkl_test_file = 'Lab1-2016-Q4Half-ST4000DM000.pkl'\nprint('Saving the train and test data sets...')\ndf.to_pickle(data_dir+pkl_train_file)\ndf_t.to_pickle(data_dir+pkl_test_file)\nprint('Completed saving data sets')","metadata":{"trusted":true,"execution":{"execution_failed":"2025-05-14T06:51:56.498Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<a name=\"5\"></a>\n## Training GPU XGBoost models with RAPIDS for Time Series","metadata":{}},{"cell_type":"markdown","source":"In this section, we will leverage the Backblaze Hard Drive SMART data to train an XGBoost model that will predict potential future failures.\n\nXGBoost will use the historical collection of SMART data to learn common feature patterns present in disk failures.  The model we create will be able to predict a failure based on the characteristics learned. To implement our XGBoost model we will be using  NVIDIA's [RAPIDS](https://rapids.ai/) which is an open source, GPU accelerated, data science platform.  RAPIDS includes cuML which is a \"scikit-learn-like\" library that contains GPU-accelerated versions of some of the most popular machine learning algorithms.\n\nIn this lab, we will be using RAPIDS to train XGBoost models.","metadata":{}},{"cell_type":"markdown","source":"<a name=\"6\"></a>\n### XGBoost\n\nXGBoost is widely known today as a \"go to\" model when working with structured data. In fact, there is a disproportionately large number of XGBoost-based winning entries of Kaggle competitions. In this lab, we introduce the model, but we do not intend to go through a rigorous mathematical formulation of XGBoost. For curious readers, we recommend reviewing of the original paper on [XGBoost](https://www.kdd.org/kdd2016/papers/files/rfp0697-chenAemb.pdf)\n\nTraditionally, in tree-based ensemble methods such as Random Forests, we train each tree independently and the predictions of multiple trees are summed to obtain the final score. The figure below classifies whether someone would like to play computer games or not. The left and right trees are classifying gamers based on different features, namely age and whether samples use computers on a daily basis. A weighted combination of such trees would result in a more accurate decision.\n\n<br>\n\n<img src=\"img/twocart.png\" style=\"margin-top:10px;\"/>\n<p style=\"text-align: center;color:gray\"> Figure 1. Tree classifier to detect computer gamers<br/>\n<i>Image credit: XGBoost: A Scalable Tree Boosting System, T. Chen and C. Guestrin</i></p>\n\n\n\nIn the case of XGBoost, an implementation of Gradient Boosted Decision Trees, we repeatedly build new models and combine them into an ensemble. Unlike Random Forests, we build trees one at a time, where each new tree helps to correct errors made by previously trained tree. The incremental training procedure is shown in figure 2.\n\n<br/><br/>\n\n\n\n<img src=\"img/Ensemble Tree.jpg\" style=\"margin-top:10px;\"/>\n<p style=\"text-align: center;color:gray\"> Figure 2. XGBoost training pipeline</p>\n\n\n\n\n\nThe loss function controls the predictive power of the model, and the XGBoost regularization term ensures simplicity and manages overfitting. Since \"Boosting\" focuses iteratively building learners for the most difficult parts of your data, it provides an efficient algorithm to deal with unbalanced datasets by strengthening the impact of the positive class. With this brief introduction to the method, we are going to further curate the data and implement the model.\n","metadata":{}},{"cell_type":"markdown","source":"<a name=\"7\"></a>\n## More on Data Preparation\n\nNow that we have filtered the dataset based on the `model` column, this column is not required anymore and can be removed. Also, the `date` and `serial_number` do not possess any useful information for our training. Later in the course, where we treat the hard drive date as time sequences, we would need to further sort the data using the `date` column. Let's start by removing these unnecessary columns at this stage.","metadata":{}},{"cell_type":"code","source":"# RAPIDS\nimport cudf \nimport xgboost as xgb","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# drop unneccessary columns\nprint(\"Dropping unneeded columns...\")\ndf = df.drop(columns=['model', 'date', 'serial_number'])\ndf_t = df_t.drop(columns=['model', 'date', 'serial_number'])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"For training/testing our model, we require labels. The `failure` column provides these class labels. We need to extract these labels as follows:","metadata":{}},{"cell_type":"code","source":"print(\"Separating Training Data Features and Labels\")\n# separate training set features from labels\ndf_train_target = pd.DataFrame(df['failure'])\n\nprint(\"Separating Test Data Features and Labels\")\n# separate test set features from labels\ndf_test_target = pd.DataFrame(df_t['failure'])\n\nprint(\"Completed\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Each SMART measure on the list of columns has a companion \"normalized\" twin column. The values of the normalized column are just the scaled version of the \"raw\" columns. We can choose to opt either of these columns for this lab, as XGBoost is not sensitive to the scale of the values of each column. Here, we choose to keep the \"raw\" version.\n\n","metadata":{}},{"cell_type":"code","source":"cols = [c for c in df.columns if c.lower().find(\"normalized\")==-1]\ndf=df[cols]\ndf.columns","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<a name=\"e1\"></a>\n## Remove \"zero-valued\" rows\n\nAnother important drawback with the data is \"zero-valued\" rows. These rows have zero values for every entry of the dataset and do not possess any useful information for training the model. We can remove these rows from the training sets.","metadata":{}},{"cell_type":"code","source":"#remove zero valued rows\ndf = df.loc[:, (df != 0).any(axis=0)]\ndf.columns","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Now that these columns are removed, we should apply the same to our test set. Note that for our test set, even if they contain non-zero values, since they are removed from the train set, they will not be useful. Also, we need to remove the `failure` column - which is our class labels! - from the training and testing sets.","metadata":{}},{"cell_type":"code","source":"cols = [c for c in df_t.columns if c.lower() in df.columns]\ndf_t=df_t[cols]\n\ndf_train = df.drop(columns=['failure'])\ndf_test = df_t.drop(columns=['failure'])\nprint(\"Completed\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Let's look at a few records from the dataset","metadata":{}},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<a name=\"8\"></a>\n## Define XGBoost Model Parameters","metadata":{}},{"cell_type":"markdown","source":"Similar to other machine learning models, there are hyperparameters to be set for the XGBoost. Below is the list of parameters of the model:\n\n- <b>max_tree_depth</b>:  Defines the maximum depth of the tree.  Deeper trees are more complex and able to learn more relations amongst the data, however, also tend to over-fit as the relations may be specific to the training set.\n\n- <b>tree_method</b>:  Used to control GPU vs CPU operation.  'hist' = CPU histogram method.  'gpu_hist' = GPU histogram method.\n\n- <b>subsample</b>:  Denotes the fraction of observations to be randomly sampled for each tree.  Lower values make the algorithm more conservative and prevent overfitting however, too small of a value might lead to underfitting.\n\n- <b>regularization</b>:  Applies an L1-regularization term to the weights.  Increasing this value makes the model more conservative and can speed up performance on very high-dimensional data sets.\n\n- <b>gamma</b>:  Gamma specifies the minimum loss reduction required to split a node.\n\n- <b>pos_weight</b>:  Controls the balance of positive and negative weights to deal with unbalanced classes.\n\n- <b>early_stop</b>:  Used to control over-fitting.  Attempts to find the inflection point when the performance on the test set starts to decrease while the performance on the training set continues to increase.","metadata":{}},{"cell_type":"code","source":"MAX_TREE_DEPTH = 8\nTREE_METHOD = 'hist'\nITERATIONS = 85\nSUBSAMPLE = 0.6\nREGULARIZATION = 1.3\nGAMMA = 0.3\nPOS_WEIGHT = 1\nEARLY_STOP = 10","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<a name=\"9\"></a>\n## Train XGBoost on CPUs\n\nTo demonstrate the benefits of GPU computing and the RAPIDS platform, we will first train a XGBoost model on the CPUs in our training enviroment. Let's take a look at the specs of the CPUs running on your cloud instance.","metadata":{}},{"cell_type":"code","source":"# Show information on the CPUs\n!lscpu | grep 'Model name:'\n!lscpu | grep 'CPU(s)'","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"First, we need to convert our data into XGBoost DMatrix format. DMatrix is optimized for memory efficiency and training speed in XGBoost. Next, we need to create a dictionary of hyperparameters discussed above and finally call the `train` method to start the training process. We also measure the total time to compare it with the GPU version.","metadata":{}},{"cell_type":"code","source":"start_time = time.time()\n\nprint('Converting data sets into XGBoost DMatrix format...')\nxgtrain = xgb.DMatrix(df_train, df_train_target)\nxgeval = xgb.DMatrix(df_test, df_test_target)\nprint('Completed converting data sets')\n\nparams = {'tree_method': TREE_METHOD, 'max_depth': MAX_TREE_DEPTH, 'alpha': REGULARIZATION,\n          'gamma': GAMMA, 'subsample': SUBSAMPLE, 'scale_pos_weight': POS_WEIGHT, 'learning_rate': 0.05, 'silent': 1}\n\nprint('Starting CPU XGBoost Training...')\nbst = xgb.train(params, xgtrain, ITERATIONS, evals=[(xgtrain, \"train\"), (xgeval, \"eval\")],\n                early_stopping_rounds=EARLY_STOP)\n\ntimetaken_cpu = time.time() - start_time\nprint('CPU XGBoost training compeleted - elapsed time:', timetaken_cpu,'seconds')\n\n# free up memory\ndel xgtrain\ndel xgeval\ndel bst","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<a name=\"10\"></a>\n## Training with GPU without cuDF DataFrame\n\nWe will now leverage GPUs to accelerate the XGBoost Training and compare the results to the CPU version.\n\nFirst, let's see which GPU we have in our lab system.","metadata":{}},{"cell_type":"code","source":"!nvidia-smi","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"To configure the XGBoost training algorithm to use GPUs, we set params[tree_method] = 'gpu_hist'","metadata":{}},{"cell_type":"code","source":"# GPU, without using cuDF\n\nstart_time = time.time()\n\nprint('Converting data sets into XGBoost DMatrix format...')\nxgtrain = xgb.DMatrix(df_train, df_train_target)\nxgeval = xgb.DMatrix(df_test, df_test_target)\nprint('Completed converting data sets')\n\nparams = {'tree_method': \"gpu_\"+TREE_METHOD, 'max_depth': MAX_TREE_DEPTH, 'alpha': REGULARIZATION,\n          'gamma': GAMMA, 'subsample': SUBSAMPLE, 'scale_pos_weight': POS_WEIGHT, 'learning_rate': 0.05, 'silent': 1}\n\nprint('Starting GPU XGBoost Training...')\nbst = xgb.train(params, xgtrain, ITERATIONS, evals=[(xgtrain, \"train\"), (xgeval, \"eval\")],\n                early_stopping_rounds=EARLY_STOP)\n\ntimetaken_gpu_nocudf = time.time() - start_time\nprint('GPU XGBoost training compeleted - elapsed time:', timetaken_gpu_nocudf,'seconds')\n\n# free up memory\ndel xgtrain\ndel xgeval\ndel bst","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<a name=\"11\"></a>\n## Training with GPU and cuDF\n\nIn this section we will be using cuDF instead of pandas DataFrame. cuDF is a GPU DataFrame library for loading, joining, aggregating, filtering, and otherwise manipulating data, which is based on Apache Arrow columnar memory format. cuDF provides a pandas-like API for familiarity which allows us to easily accelerate workflows without going into the details of CUDA programming. For more information on cuDF, please refer to [RAPIDS Github Repositor](https://github.com/rapidsai/cudf).\nTo use cuDF with the XGBoost algorithm, we load the Pandas Dataframe into a cuDF Dataframe (Python object type cudf.dataframe.dataframe.DataFrame) first.\n","metadata":{}},{"cell_type":"code","source":"# load into cuDF Dataframe\n\ngdf_train = cudf.DataFrame.from_pandas(df_train)\ngdf_train_target = cudf.DataFrame.from_pandas(df_train_target)\n\ngdf_eval = cudf.DataFrame.from_pandas(df_test)\ngdf_eval_target = cudf.DataFrame.from_pandas(df_test_target)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Now, we are going to measure the XGBoost training time using the created cuDF and on GPU.","metadata":{}},{"cell_type":"code","source":"# GPU, with cuDF\n\nstart_time = time.time()\n\nprint('Converting cuDF data sets into XGBoost DMatrix format...')\nxgtrain = xgb.DMatrix(gdf_train, gdf_train_target)\nxgeval = xgb.DMatrix(gdf_eval, gdf_eval_target)\nprint('Completed converting data sets')\n\nparams = {'tree_method': \"gpu_\"+TREE_METHOD, 'max_depth': MAX_TREE_DEPTH, 'alpha': REGULARIZATION,\n          'gamma': GAMMA, 'subsample': SUBSAMPLE, 'scale_pos_weight': POS_WEIGHT, 'learning_rate': 0.05, 'silent': 1}\n\nprint('Starting GPU XGBoost Training with cuDF Dataframes...')\nbst = xgb.train(params, xgtrain, ITERATIONS, evals=[(xgtrain, \"train\"), (xgeval, \"eval\")],\n                early_stopping_rounds=EARLY_STOP)\n\ntimetaken_gpu = time.time() - start_time\nprint('GPU XGBoost Training with cuDF Dataframes compeleted - elapsed time:', timetaken_gpu,'seconds')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Let's visualize the training time for the three configurations described above. ","metadata":{}},{"cell_type":"code","source":"labels = 'CPU', 'GPU', 'GPU+cuDF'\nsizes = [timetaken_cpu, timetaken_gpu_nocudf, timetaken_gpu]\n\nplt.style.use('ggplot')\n\nx_pos = [i for i, _ in enumerate(labels)]\nplt.bar(x_pos, sizes)\nplt.xlabel(\"XGBoost Version\")\nplt.ylabel(\"Time (secs)\")\nplt.title(\"Training Time\")\nplt.xticks(x_pos, labels)\n\nplt.show()\n\nprint(\"CPU Time Taken:\\n\", round(timetaken_cpu,1),\"seconds\")\nprint(\"\\nGPU (no cuDF) Time Taken:\\n\", round(timetaken_gpu_nocudf,1),\"seconds\")\nprint(\"\\nGPU (cuDF) Time Taken:\\n\", round(timetaken_gpu,1),\"seconds\")\nprint(\"\\nTotal speed-up with RAPIDS:\\n\", round(timetaken_cpu/timetaken_gpu*100,1), \"%\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"As you can see, using the GPU + cuDF we see a significant speed-up. In the next section, we are going to explore the accuracy of the XGBoost model.","metadata":{}},{"cell_type":"markdown","source":"<a name=\"12\"></a>\n### XGBoost Model Accuracy\n\nLet's look at the model's accuracy on the evalutation set that we will use to compare against our Deep Learning models","metadata":{}},{"cell_type":"code","source":"print('Starting XGBoost prediction on test dataset...')\n\n# Use trained model to predict on test data set\npreds = bst.predict(xgeval)\n\n# Convert predictions to \"normal\" (0) or \"failed\"\ny_pred = []\n\nTHRESHOLD = 0.5\nfor pred in preds:\n    if pred<=THRESHOLD:\n        y_pred.append(0)\n    if pred>THRESHOLD:\n        y_pred.append(1)\n\n# Output array of classifications        \ny_pred = np.asarray(y_pred)\n        \ny_true = df_test_target.values.reshape(len(preds))\n\nprint('XGBoost prediction completed')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Accuracy (Eval)\", round(accuracy_score(y_true, y_pred),5))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<a name=\"13\"></a>\n## Measuring Model Accuracy\n\nIn this section, we are going to plot the confusion matrix. While accuracy rate provides us with an overall outlook on how well our model is performing, we need more insights into the accuracy. Especially, since our dataset is small, and we need to compromise some aspects of accuracy rate in favor of others. The `classification_report` provides us with the accuracy stats broken down into `precision`, `recall`, `f1-score` and `support`. Before introducing these measures, let's define some abbreviations:\n\nFor brevity we are using the following abbreviations: \n\n\n- __True Positive (TP)__:  test correctly classifies the input to the class (hard drive failed, and we classified it as fail)\n- __False Positive (FP)__:  test incorrectly classifies the input to the class (hard drive is normal, but we classified it as fail)\n- __False Negative (FN)__:  test incorrectly misses to classify the input to the class (hard drive failed but we classified as normal)\n- __True Negative (TN)__:  test correctly misses to classify the input to the class (hard drive is normal, and we classified as normal)\n\nThe first measure is focused on identifying positive cases and is called __recall__. We define recall as the ability of the model to identify all true positive samples of the dataset. In mathematical terms, recall is the ratio of true positives over true positives plus false negatives. By other means, recall tells us, among all the test samples belonging to the output class, how many of them are identified correctly by the model:\n\n\\begin{equation*}\nrecall = \\frac{TP}{TP+FN}\n\\end{equation*}\n\nThe next measure, is called __precision__ and is the ability of the model to identify the relevant samples only, and is defined as the ratio of true positives over true positives plus false positives:\n\n\\begin{equation*}\nprecision = \\frac{TP}{TP+FP}\n\\end{equation*}\n\n\nSelecting a proper threshold, usually stems from a good balance between the precision and recall values. A well-known measure that provides such a balance is `F1 score`, which is a harmonic mean of precision and recall, and defined as:\n\n\\begin{equation*}\n{F_1 \\: score} = 2*\\frac{precision*recall}{precision+recall}\n\\end{equation*}\n\nF1 Score is generally considered a better overall measurement than accuracy when an uneven class distribution exists (large number of True Negatives) as in the case of predicting a hard drive failure.\n\nOne very important measure is the recall rate on defective samples. We are interested in models that retrieve as many defective hard drives as possible. We try to predict at least half of the defective hard drives. Let's measure how our model is performing:","metadata":{}},{"cell_type":"code","source":"print(classification_report(y_true, y_pred, target_names=[\"normal\", \"fail\"]))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<a name=\"14\"></a>\n## Interpreting the results\n\nWhen trying to predict hard drive failure, we can interpret the classification report above in this way:\n\n- Precision is the percentage of times the model is correct when predicting the given class.\n- Recall is the percentage of times we predicted that class against the total number of instances of the class in the data set.\n\nTherefore, the classification report above is telling us that the model predicted a failure for ~67% of the 234 fail sequences, and was correct 3% of the time. As for normal hard drives the recall rate is 1.00 and we are accurate ~100% of the time. \n\n<b>note</b>:  your precision / recall percentages may vary slightly depending on the weights learned during training","metadata":{}},{"cell_type":"markdown","source":"## <b>Group Discussion</b>\n\nWhat do the results above tell us and any ideas how to improve?","metadata":{}},{"cell_type":"code","source":"print('Summary of training data:')\ntrain_all_count = df.shape[0]\ntrain_fail_count = df[df['failure'] >= 1].shape[0]\nprint ('- Number of disks : ', train_all_count)\nprint ('- Number of failed disks: ', train_fail_count)\nprint ('- Percentage of failed disks: %.4f' %(train_fail_count/train_all_count*100),'%' )","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print('Summary of test data:')\ntest_all_count = df_t.shape[0]\ntest_fail_count = df_t[df_t['failure'] >= 1].shape[0]\nprint ('- Number of disks : ', test_all_count)\nprint ('- Number of failed disks: ', test_fail_count)\nprint ('- Percentage of failed disks: %.4f' %(test_fail_count/test_all_count*100),'%' )","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Our training and test sets have large class imbalances between normal and failed disks.  During the training process, our model is trying to find the optimal function for the lowest error loss.  Naively, if the model classifies every hard drive as normal, then the model is right more than 99% of the time.\n\nThus, we need to come up with a strategy to address this imbalance. Some strategies to resolve class imbalance data include:","metadata":{}},{"cell_type":"markdown","source":"- Up / Down Sample Classes\n- Weighted Class Cost Optimization\n- Evaluate various error metrics\n\n\nIn the next section, we are going to resample the \"normal\" class to match the number of samples within both \"normal\" and \"failure\" classes of the dataset.","metadata":{}},{"cell_type":"markdown","source":"<a name=\"15\"></a>\n## Dealing with Imbalanced Data\n\nWhen the data samples do not equally represent their classes, it is referred to as \"imbalanced data\" problem. In our example, we have __3196318__ samples of the normal hard drives, while this number for the \"failure\" class is only __234__. This clearly results in a model that is biased towards the \"normal\" samples, and the gradients of the \"failure\" class tend to vanish during the training epochs.\n\n\n```\n              precision    recall  f1-score   support\n\n      normal       1.00      1.00      1.00   3196318\n        fail       0.67      0.03      0.05       234\n```\n\n\nTo mitigate the issue, in the next section we are going to resample the \"normal\" class to be roughly equal to the number of samples in the \"failure\" class (by a factor of 1.2).","metadata":{}},{"cell_type":"code","source":"# Sampling training and test data sets for smaller, more balanced test\nnum_normal_delta = round(train_fail_count * 0.2)\nnum_normal_test_delta = round(test_fail_count * 0.2)\n\n# Sample training data\ndf_tmp = df[df['failure'] > 0]\nsample_train_count_failed = df_tmp.shape[0]\ndf_tmp = df_tmp.append(df[df['failure'] == 0].sample(n=(df_tmp.shape[0]+num_normal_delta)))\nsample_train_count_normal = df_tmp.shape[0]-sample_train_count_failed\n\n# separate training set features from labels\ndf_train = df_tmp.drop(columns=['failure'])\ndf_train_target = pd.DataFrame(df_tmp['failure'])\n\n# Sample test data\ndf_tmp = df_t[df_t['failure'] > 0]\nsample_test_count_failed = df_tmp.shape[0]\ndf_tmp = df_tmp.append(df_t[df_t['failure'] == 0].sample(n=(df_tmp.shape[0]+num_normal_test_delta)))\nsample_test_count_normal = df_tmp.shape[0]-sample_test_count_failed\n\n# separate test set features from labels\ndf_test = df_tmp.drop(columns=['failure'])\ndf_test_target = pd.DataFrame(df_tmp['failure'])\n\n# Output name data set sizes\nprint('Sampled Dataset Sizes:')\nprint('- Train: Number of Normal Disks :',sample_train_count_normal)\nprint('- Train: Number of Failed Disks :',sample_train_count_failed)\nprint('- Test:  Number of Normal Disks :',sample_test_count_normal)\nprint('- Test:  Number of Failed Disks :',sample_test_count_failed)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<a name=\"e2\"></a>\n### Exercise 2: Implications of balancing data samples\n\nBefore proceeding with training, take some time and think about the implications of rebalancing the data. we have down-sampled the \"normal\" data points from 3M to ~320, a 10-thousand-time reduction in the number of samples. \n* How do you expect this down-sampling to affect each class? \n* What would be the effect on the precision/recall and f1-score values for each class? \n\nWrite your answers below before training the model:","metadata":{}},{"cell_type":"raw","source":"Downsampling the dominant class will increase the overall perfomance of the model, although we will see a decrease in the final accuracy value. This is one of the reasons why accuracy isn't an appropriate evaluation metric for problems with an imbalanced dataset.\nThe precision, recall and F1 Score values for the \"fail\" class should increase dramatically, while the values for the \"normal\" class should decrease slightly.","metadata":{}},{"cell_type":"markdown","source":"Next, we are going to build the cuDF data from the pandas DataFrame","metadata":{}},{"cell_type":"code","source":"# load sampled data into cuDF Dataframe\n\ngdf_train = cudf.DataFrame.from_pandas(df_train)\ngdf_train_target = cudf.DataFrame.from_pandas(df_train_target)\n\ngdf_eval = cudf.DataFrame.from_pandas(df_test)\ngdf_eval_target = cudf.DataFrame.from_pandas(df_test_target)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"And finally perform training on the resampled training data","metadata":{}},{"cell_type":"code","source":"# GPU, with cuDF\n\nstart_time = time.time()\n\nprint('Converting cuDF data sets into XGBoost DMatrix format...')\nxgtrain = xgb.DMatrix(gdf_train, gdf_train_target)\nxgeval = xgb.DMatrix(gdf_eval, gdf_eval_target)\nprint('Completed converting data sets')\n\nparams = {'tree_method': \"gpu_\"+TREE_METHOD, 'max_depth': MAX_TREE_DEPTH, 'alpha': REGULARIZATION,\n          'gamma': GAMMA, 'subsample': SUBSAMPLE, 'scale_pos_weight': POS_WEIGHT, 'learning_rate': 0.05, 'silent': 1}\n\nprint('Starting GPU XGBoost Training with cuDF Dataframes...')\nbst = xgb.train(params, xgtrain, 20, evals=[(xgtrain, \"train\"), (xgeval, \"eval\")],\n                early_stopping_rounds=EARLY_STOP)\n\ntimetaken_gpu = time.time() - start_time\nprint('GPU XGBoost Training with cuDF Dataframes compeleted - elapsed time:', timetaken_gpu,'seconds')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Now that the model is trained, let's run it over the test set.","metadata":{}},{"cell_type":"code","source":"print('Starting XGBoost prediction on test dataset...')\n\n# Use trained model to predict on test data set\npreds = bst.predict(xgeval)\n\n# Convert predictions to \"normal\" (0) or \"failed\"\ny_pred = []\nTHRESHOLD = 0.5\n\nfor pred in preds:\n    if pred<=THRESHOLD:\n        y_pred.append(0)\n    if pred>THRESHOLD:\n        y_pred.append(1)\n\n# Output array of classifications                \ny_pred = np.asarray(y_pred)\n        \ny_true = df_test_target.values.reshape(len(preds))\nprint('XGBoost prediction completed')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"And finally review accuracy with the new results.","metadata":{}},{"cell_type":"code","source":"print(\"Accuracy (Eval)\", round(accuracy_score(y_true, y_pred),5))\n\nprint(classification_report(y_true, y_pred, target_names=[\"normal\", \"fail\"]))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"As you can see, the recall/precision values for the \"fail\" class are hugely improved. The cost we pay here is the reduction in the precision/recall values of the \"normal\" class. Could you explain some scenarios where each of these strategies would be applicable? \nFinally, Compare the obtained results with your answers to exercise 2. How closely were you able to predict the results?\n","metadata":{}},{"cell_type":"markdown","source":"## Summary\n\nIn this lab, we learned how to work with a sample dataset and curate, normalize, and filter data to prepare it for our XGBoost model. We reviewed the XGBoost model and learned how to utilize the cuDF data structure to accelerate the training/testing phases for the XGBoost model. Moreover, we learned how to modify the data to mitigate the issue of imbalanced classes.\nIn the next two labs, we will use deep-learning-based approaches for classification and prediction.","metadata":{}}]}